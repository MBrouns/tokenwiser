{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"tokenwiser \u00b6 Bag of, not words, but tricks! This project contains a couple of useful \"tricks\" on tokens. It's a collection of tricks for sparse data that might be trained on a stream of data. Goal \u00b6 We noticed that a lot of benchmarks relied on heavy-weight tools while they did not check if something more lightweight would also work. Maybe if we just apply some simple tricks on our tokens we won't need massive language models. The goal of this package is to contribute tricks to keep your NLP pipelines simple but also some general tools that are useful in scikit-learn and spaCy. If you're looking for a tool that can add language models to scikit-learn pipelines as a benchmark you'll want to explore another tool: whatlies . Features \u00b6 Scikit-Learn Tools \u00b6 The following submodules contain features that might be useful. .textprep : Contains string pre-processing tools for scikit-learn. Takes a string in and pushes a string out. .pipeline : Contains extra pipeline components for scikit-learn to make it easier to work with strings and partial models. SpaCy Tools \u00b6 .component : Contains spaCy compatible components that might be added as a pipeline step. .extension : Contains spaCy compatible extensions that might be added manually.","title":"Home"},{"location":"index.html#tokenwiser","text":"Bag of, not words, but tricks! This project contains a couple of useful \"tricks\" on tokens. It's a collection of tricks for sparse data that might be trained on a stream of data.","title":"tokenwiser"},{"location":"index.html#goal","text":"We noticed that a lot of benchmarks relied on heavy-weight tools while they did not check if something more lightweight would also work. Maybe if we just apply some simple tricks on our tokens we won't need massive language models. The goal of this package is to contribute tricks to keep your NLP pipelines simple but also some general tools that are useful in scikit-learn and spaCy. If you're looking for a tool that can add language models to scikit-learn pipelines as a benchmark you'll want to explore another tool: whatlies .","title":"Goal"},{"location":"index.html#features","text":"","title":"Features"},{"location":"index.html#scikit-learn-tools","text":"The following submodules contain features that might be useful. .textprep : Contains string pre-processing tools for scikit-learn. Takes a string in and pushes a string out. .pipeline : Contains extra pipeline components for scikit-learn to make it easier to work with strings and partial models.","title":"Scikit-Learn Tools"},{"location":"index.html#spacy-tools","text":".component : Contains spaCy compatible components that might be added as a pipeline step. .extension : Contains spaCy compatible extensions that might be added manually.","title":"SpaCy Tools"},{"location":"goal.html","text":"We noticed that a lot of benchmarks relied on heavy-weight tools while they did not check if something more lightweight would also work. In this package we list tools that will help you do lightweight benchmarks using three packages: scikit-learn spaCy vowpal wabbit The idea is that maybe we don't need heavy language models for basic tasks. Maybe we just need to apply some simple tricks on our tokens. The goal of this package is to contribute tricks to keep your NLP pipelines simple. ps. If you're looking for a tool that can add language models to scikit-learn pipelines as a benchmark you'll want to explore another tool: whatlies .","title":"Goal"},{"location":"snippets.html","text":"Here's a few useful snippets to keep around. Scikit-Learn in SpaCy \u00b6 You can add a custom component if you're really keen. import spacy import pandas as pd from spacy.language import Language from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline , make_union df = pd . read_json ( \"data/intents.jsonl\" , lines = True ) feat = make_union ( CountVectorizer ()) pipe = make_pipeline ( feat , LogisticRegression ()) pipe . fit ( df [ 'text' ], df [ 'label' ]) nlp = spacy . load ( \"en_core_web_md\" ) @Language . component ( \"sklearn-cat\" ) def my_component ( doc ): pred = pipe . predict ([ doc . text ])[ 0 ] proba = pipe . predict_proba ([ doc . text ]) . max () doc . cats [ pred ] = proba return doc nlp . add_pipe ( \"sklearn-cat\" ) nlp ( \"can you flip a coin?\" ) . cats # {'flip_coin': 0.9747356912446946}","title":"Snippets"},{"location":"snippets.html#scikit-learn-in-spacy","text":"You can add a custom component if you're really keen. import spacy import pandas as pd from spacy.language import Language from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline , make_union df = pd . read_json ( \"data/intents.jsonl\" , lines = True ) feat = make_union ( CountVectorizer ()) pipe = make_pipeline ( feat , LogisticRegression ()) pipe . fit ( df [ 'text' ], df [ 'label' ]) nlp = spacy . load ( \"en_core_web_md\" ) @Language . component ( \"sklearn-cat\" ) def my_component ( doc ): pred = pipe . predict ([ doc . text ])[ 0 ] proba = pipe . predict_proba ([ doc . text ]) . max () doc . cats [ pred ] = proba return doc nlp . add_pipe ( \"sklearn-cat\" ) nlp ( \"can you flip a coin?\" ) . cats # {'flip_coin': 0.9747356912446946}","title":"Scikit-Learn in SpaCy"},{"location":"api/component.html","text":"component \u00b6 from tokenwiser.component import * In the component submodule you can find spaCy compatible components. attach_sklearn_categoriser ( nlp , pipe_name , estimator ) \u00b6 This function will attach a scikit-learn compatible estimator to the pipeline which will feed predictions to the .cats property. Usage: import spacy from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.component import attach_sklearn_categoriser X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = [ \"pos\" , \"pos\" , \"pos\" , \"neg\" , \"neg\" , \"neg\" ] pipe = make_pipeline ( CountVectorizer (), LogisticRegression ()) . fit ( X , y ) nlp = spacy . load ( \"en_core_web_sm\" ) attach_sklearn_categoriser ( nlp , pipe_name = \"silly_sentiment\" , estimator = pipe ) assert nlp . pipe_names [ - 1 ] == \"silly_sentiment\" assert nlp ( \"this post i really like\" ) . cats [ \"pos\" ] > 0.5 Source code in tokenwiser/component/_sklearn.py def attach_sklearn_categoriser ( nlp , pipe_name , estimator ): \"\"\" This function will attach a scikit-learn compatible estimator to the pipeline which will feed predictions to the `.cats` property. Usage: ```python import spacy from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.component import attach_sklearn_categoriser X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = [\"pos\", \"pos\", \"pos\", \"neg\", \"neg\", \"neg\"] pipe = make_pipeline(CountVectorizer(), LogisticRegression()).fit(X, y) nlp = spacy.load(\"en_core_web_sm\") attach_sklearn_categoriser(nlp, pipe_name=\"silly_sentiment\", estimator=pipe) assert nlp.pipe_names[-1] == \"silly_sentiment\" assert nlp(\"this post i really like\").cats[\"pos\"] > 0.5 ``` \"\"\" @Language . component ( pipe_name ) def my_component ( doc ): pred = estimator . predict ([ doc . text ])[ 0 ] proba = estimator . predict_proba ([ doc . text ]) . max () doc . cats [ pred ] = proba return doc nlp . add_pipe ( pipe_name )","title":"component"},{"location":"api/component.html#component","text":"from tokenwiser.component import * In the component submodule you can find spaCy compatible components.","title":"component"},{"location":"api/component.html#tokenwiser.component._sklearn.attach_sklearn_categoriser","text":"This function will attach a scikit-learn compatible estimator to the pipeline which will feed predictions to the .cats property. Usage: import spacy from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.component import attach_sklearn_categoriser X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = [ \"pos\" , \"pos\" , \"pos\" , \"neg\" , \"neg\" , \"neg\" ] pipe = make_pipeline ( CountVectorizer (), LogisticRegression ()) . fit ( X , y ) nlp = spacy . load ( \"en_core_web_sm\" ) attach_sklearn_categoriser ( nlp , pipe_name = \"silly_sentiment\" , estimator = pipe ) assert nlp . pipe_names [ - 1 ] == \"silly_sentiment\" assert nlp ( \"this post i really like\" ) . cats [ \"pos\" ] > 0.5 Source code in tokenwiser/component/_sklearn.py def attach_sklearn_categoriser ( nlp , pipe_name , estimator ): \"\"\" This function will attach a scikit-learn compatible estimator to the pipeline which will feed predictions to the `.cats` property. Usage: ```python import spacy from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.component import attach_sklearn_categoriser X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = [\"pos\", \"pos\", \"pos\", \"neg\", \"neg\", \"neg\"] pipe = make_pipeline(CountVectorizer(), LogisticRegression()).fit(X, y) nlp = spacy.load(\"en_core_web_sm\") attach_sklearn_categoriser(nlp, pipe_name=\"silly_sentiment\", estimator=pipe) assert nlp.pipe_names[-1] == \"silly_sentiment\" assert nlp(\"this post i really like\").cats[\"pos\"] > 0.5 ``` \"\"\" @Language . component ( pipe_name ) def my_component ( doc ): pred = estimator . predict ([ doc . text ])[ 0 ] proba = estimator . predict_proba ([ doc . text ]) . max () doc . cats [ pred ] = proba return doc nlp . add_pipe ( pipe_name )","title":"attach_sklearn_categoriser()"},{"location":"api/extension.html","text":"extension \u00b6 from tokenwiser.extension import * In the extension submodule you can find spaCy compatible extensions. attach_hyphen_extension () \u00b6 This function will attach an extension ._.hyphen to the Token s. import spacy from tokenwiser.extension import attach_hyphen_extension nlp = spacy . load ( \"en_core_web_sm\" ) # Attach the Hyphen extensions. attach_hyphen_extension () # Now you can query hyphens on the tokens. doc = nlp ( \"this is a dinosaurhead\" ) tok = doc [ - 1 ] assert tok . _ . hyphen == [ \"di\" , \"no\" , \"saur\" , \"head\" ] Source code in tokenwiser/extension/_extension.py def attach_hyphen_extension (): \"\"\" This function will attach an extension `._.hyphen` to the `Token`s. ```python import spacy from tokenwiser.extension import attach_hyphen_extension nlp = spacy.load(\"en_core_web_sm\") # Attach the Hyphen extensions. attach_hyphen_extension() # Now you can query hyphens on the tokens. doc = nlp(\"this is a dinosaurhead\") tok = doc[-1] assert tok._.hyphen == [\"di\", \"no\", \"saur\", \"head\"] ``` \"\"\" Token . set_extension ( \"hyphen\" , getter = lambda t : HyphenTextPrep () . encode_single ( t . text ) . split ( \" \" ), force = True , ) sklearn_method ( estimator ) \u00b6 A helper to turn a scikit-learn estimator into a spaCy extension. Just in case you really wanted to do it manually. import spacy from spacy.tokens import Doc from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.extension import sklearn_method X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = [ \"pos\" , \"pos\" , \"pos\" , \"neg\" , \"neg\" , \"neg\" ] # First we train a (silly) model. mod = make_pipeline ( CountVectorizer (), LogisticRegression ()) . fit ( X , y ) # This is where we attach the scikit-learn model to spaCy as a method extension. Doc . set_extension ( \"sillysent_method\" , method = sklearn_method ( mod )) # This is where we attach the scikit-learn model to spaCy as a property extension. Doc . set_extension ( \"sillysent_prop\" , getter = sklearn_method ( mod )) # Demo nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( \"thank you, really nice\" ) doc . _ . sillysent_method () # {\"neg\": 0.4446964938410244, \"pos: 0.5553035061589756} doc . _ . sillysent_prop # {\"neg: 0.4446964938410244, \"pos\": 0.5553035061589756} Source code in tokenwiser/extension/_extension.py def sklearn_method ( estimator ): \"\"\" A helper to turn a scikit-learn estimator into a spaCy extension. Just in case you *really* wanted to do it manually. ```python import spacy from spacy.tokens import Doc from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.extension import sklearn_method X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = [\"pos\", \"pos\", \"pos\", \"neg\", \"neg\", \"neg\"] # First we train a (silly) model. mod = make_pipeline(CountVectorizer(), LogisticRegression()).fit(X, y) # This is where we attach the scikit-learn model to spaCy as a method extension. Doc.set_extension(\"sillysent_method\", method=sklearn_method(mod)) # This is where we attach the scikit-learn model to spaCy as a property extension. Doc.set_extension(\"sillysent_prop\", getter=sklearn_method(mod)) # Demo nlp = spacy.load(\"en_core_web_sm\") doc = nlp(\"thank you, really nice\") doc._.sillysent_method() # {\"neg\": 0.4446964938410244, \"pos: 0.5553035061589756} doc._.sillysent_prop # {\"neg: 0.4446964938410244, \"pos\": 0.5553035061589756} ``` \"\"\" def method ( doc ): proba = estimator . predict_proba ([ doc . text ])[ 0 ] return { c : p for c , p in zip ( estimator . classes_ , proba )} return method","title":"extension"},{"location":"api/extension.html#extension","text":"from tokenwiser.extension import * In the extension submodule you can find spaCy compatible extensions.","title":"extension"},{"location":"api/extension.html#tokenwiser.extension._extension.attach_hyphen_extension","text":"This function will attach an extension ._.hyphen to the Token s. import spacy from tokenwiser.extension import attach_hyphen_extension nlp = spacy . load ( \"en_core_web_sm\" ) # Attach the Hyphen extensions. attach_hyphen_extension () # Now you can query hyphens on the tokens. doc = nlp ( \"this is a dinosaurhead\" ) tok = doc [ - 1 ] assert tok . _ . hyphen == [ \"di\" , \"no\" , \"saur\" , \"head\" ] Source code in tokenwiser/extension/_extension.py def attach_hyphen_extension (): \"\"\" This function will attach an extension `._.hyphen` to the `Token`s. ```python import spacy from tokenwiser.extension import attach_hyphen_extension nlp = spacy.load(\"en_core_web_sm\") # Attach the Hyphen extensions. attach_hyphen_extension() # Now you can query hyphens on the tokens. doc = nlp(\"this is a dinosaurhead\") tok = doc[-1] assert tok._.hyphen == [\"di\", \"no\", \"saur\", \"head\"] ``` \"\"\" Token . set_extension ( \"hyphen\" , getter = lambda t : HyphenTextPrep () . encode_single ( t . text ) . split ( \" \" ), force = True , )","title":"attach_hyphen_extension()"},{"location":"api/extension.html#tokenwiser.extension._extension.sklearn_method","text":"A helper to turn a scikit-learn estimator into a spaCy extension. Just in case you really wanted to do it manually. import spacy from spacy.tokens import Doc from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.extension import sklearn_method X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = [ \"pos\" , \"pos\" , \"pos\" , \"neg\" , \"neg\" , \"neg\" ] # First we train a (silly) model. mod = make_pipeline ( CountVectorizer (), LogisticRegression ()) . fit ( X , y ) # This is where we attach the scikit-learn model to spaCy as a method extension. Doc . set_extension ( \"sillysent_method\" , method = sklearn_method ( mod )) # This is where we attach the scikit-learn model to spaCy as a property extension. Doc . set_extension ( \"sillysent_prop\" , getter = sklearn_method ( mod )) # Demo nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( \"thank you, really nice\" ) doc . _ . sillysent_method () # {\"neg\": 0.4446964938410244, \"pos: 0.5553035061589756} doc . _ . sillysent_prop # {\"neg: 0.4446964938410244, \"pos\": 0.5553035061589756} Source code in tokenwiser/extension/_extension.py def sklearn_method ( estimator ): \"\"\" A helper to turn a scikit-learn estimator into a spaCy extension. Just in case you *really* wanted to do it manually. ```python import spacy from spacy.tokens import Doc from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.extension import sklearn_method X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = [\"pos\", \"pos\", \"pos\", \"neg\", \"neg\", \"neg\"] # First we train a (silly) model. mod = make_pipeline(CountVectorizer(), LogisticRegression()).fit(X, y) # This is where we attach the scikit-learn model to spaCy as a method extension. Doc.set_extension(\"sillysent_method\", method=sklearn_method(mod)) # This is where we attach the scikit-learn model to spaCy as a property extension. Doc.set_extension(\"sillysent_prop\", getter=sklearn_method(mod)) # Demo nlp = spacy.load(\"en_core_web_sm\") doc = nlp(\"thank you, really nice\") doc._.sillysent_method() # {\"neg\": 0.4446964938410244, \"pos: 0.5553035061589756} doc._.sillysent_prop # {\"neg: 0.4446964938410244, \"pos\": 0.5553035061589756} ``` \"\"\" def method ( doc ): proba = estimator . predict_proba ([ doc . text ])[ 0 ] return { c : p for c , p in zip ( estimator . classes_ , proba )} return method","title":"sklearn_method()"},{"location":"api/pipeline.html","text":"pipeline \u00b6 from tokenwiser.pipeline import * In the pipeline submodule you can find scikit-learn compatbile pipelines that extend the standard behavior. PartialPipeline \u00b6 Utility function to generate a PartialPipeline Parameters: Name Type Description Default steps a collection of text-transformers required from tokenwiser.pipeline import PartialPipeline from tokenwiser.textprep import HyphenTextPrep , Cleaner tc = PartialPipeline ([( 'clean' , Cleaner ()), ( 'hyp' , HyphenTextPrep ())]) data = [ \"dinosaurhead\" , \"another$$ sentence$$\" ] results = tc . fit_partial ( data ) . transform ( data ) expected = [ 'di no saur head' , 'an other sen tence' ] assert results == expected fit_partial ( self , X , y = None ) \u00b6 Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_pipe.py def fit_partial ( self , X , y = None ): \"\"\" Fits the components, but allow for batches. \"\"\" for name , step in self . steps : if not hasattr ( step , \"fit_partial\" ): raise ValueError ( f \"Step { name } is a { step } which does not have `.fit_partial` implemented.\" ) for name , step in self . steps : step . fit_partial ( X , y ) return self TextConcat \u00b6 A component like FeatureUnion but this also concatenates the text. Parameters: Name Type Description Default transformer_list list of (name, text-transformer)-tuples required Examples: from tokenwiser.textprep import HyphenTextPrep , Cleaner from tokenwiser.pipeline import TextConcat tc = TextConcat ([( \"hyp\" , HyphenTextPrep ()), ( \"clean\" , Cleaner ())]) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected fit ( self , X , y = None ) \u00b6 Fits the components in a single batch. Source code in tokenwiser/pipeline/_concat.py def fit ( self , X , y = None ): \"\"\" Fits the components in a single batch. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) return self fit_partial ( self , X , y = None ) \u00b6 Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_concat.py def fit_partial ( self , X , y = None ): \"\"\" Fits the components, but allow for batches. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) return self fit_transform ( self , X , y = None ) \u00b6 Fits the components and transforms the text in one step. Source code in tokenwiser/pipeline/_concat.py def fit_transform ( self , X , y = None ): \"\"\" Fits the components and transforms the text in one step. \"\"\" return self . fit ( X , y ) . transform ( X , y ) transform ( self , X , y = None ) \u00b6 Transformers the text. Source code in tokenwiser/pipeline/_concat.py def transform ( self , X , y = None ): \"\"\" Transformers the text. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) results = {} for name , tfm in self . transformer_list : results [ name ] = tfm . transform ( X ) return [ \" \" . join ([ results [ n ][ i ] for n in names ]) for i in range ( len ( X ))] make_partial_pipeline ( * steps ) \u00b6 Utility function to generate a PartialPipeline Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.pipeline import make_partial_pipeline from tokenwiser.textprep import HyphenTextPrep , Cleaner tc = make_partial_pipeline ( Cleaner (), HyphenTextPrep ()) data = [ \"dinosaurhead\" , \"another$$ sentence$$\" ] results = tc . fit_partial ( data ) . transform ( data ) expected = [ 'di no saur head' , 'an other sen tence' ] assert results == expected Source code in tokenwiser/pipeline/_pipe.py def make_partial_pipeline ( * steps ): \"\"\" Utility function to generate a `PartialPipeline` Arguments: steps: a collection of text-transformers ```python from tokenwiser.pipeline import make_partial_pipeline from tokenwiser.textprep import HyphenTextPrep, Cleaner tc = make_partial_pipeline(Cleaner(), HyphenTextPrep()) data = [\"dinosaurhead\", \"another$$ sentence$$\"] results = tc.fit_partial(data).transform(data) expected = ['di no saur head', 'an other sen tence'] assert results == expected ``` \"\"\" return PartialPipeline ( _name_estimators ( steps )) make_concat ( * steps ) \u00b6 Utility function to generate a TextConcat Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.textprep import HyphenTextPrep , Cleaner from tokenwiser.pipeline import make_concat tc = make_concat ( HyphenTextPrep (), Cleaner ()) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected Source code in tokenwiser/pipeline/_concat.py def make_concat ( * steps ): \"\"\" Utility function to generate a `TextConcat` Arguments: steps: a collection of text-transformers ```python from tokenwiser.textprep import HyphenTextPrep, Cleaner from tokenwiser.pipeline import make_concat tc = make_concat(HyphenTextPrep(), Cleaner()) results = tc.fit_transform([\"dinosaurhead\", \"another$$ sentence$$\"]) expected = ['di no saur head dinosaurhead', 'an other $$ sen tence$$ another sentence'] assert results == expected ``` \"\"\" return TextConcat ( _name_estimators ( steps ))","title":"pipeline"},{"location":"api/pipeline.html#pipeline","text":"from tokenwiser.pipeline import * In the pipeline submodule you can find scikit-learn compatbile pipelines that extend the standard behavior.","title":"pipeline"},{"location":"api/pipeline.html#tokenwiser.pipeline._pipe.PartialPipeline","text":"Utility function to generate a PartialPipeline Parameters: Name Type Description Default steps a collection of text-transformers required from tokenwiser.pipeline import PartialPipeline from tokenwiser.textprep import HyphenTextPrep , Cleaner tc = PartialPipeline ([( 'clean' , Cleaner ()), ( 'hyp' , HyphenTextPrep ())]) data = [ \"dinosaurhead\" , \"another$$ sentence$$\" ] results = tc . fit_partial ( data ) . transform ( data ) expected = [ 'di no saur head' , 'an other sen tence' ] assert results == expected","title":"PartialPipeline"},{"location":"api/pipeline.html#tokenwiser.pipeline._pipe.PartialPipeline.fit_partial","text":"Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_pipe.py def fit_partial ( self , X , y = None ): \"\"\" Fits the components, but allow for batches. \"\"\" for name , step in self . steps : if not hasattr ( step , \"fit_partial\" ): raise ValueError ( f \"Step { name } is a { step } which does not have `.fit_partial` implemented.\" ) for name , step in self . steps : step . fit_partial ( X , y ) return self","title":"fit_partial()"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.TextConcat","text":"A component like FeatureUnion but this also concatenates the text. Parameters: Name Type Description Default transformer_list list of (name, text-transformer)-tuples required Examples: from tokenwiser.textprep import HyphenTextPrep , Cleaner from tokenwiser.pipeline import TextConcat tc = TextConcat ([( \"hyp\" , HyphenTextPrep ()), ( \"clean\" , Cleaner ())]) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected","title":"TextConcat"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.TextConcat.fit","text":"Fits the components in a single batch. Source code in tokenwiser/pipeline/_concat.py def fit ( self , X , y = None ): \"\"\" Fits the components in a single batch. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) return self","title":"fit()"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.TextConcat.fit_partial","text":"Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_concat.py def fit_partial ( self , X , y = None ): \"\"\" Fits the components, but allow for batches. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) return self","title":"fit_partial()"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.TextConcat.fit_transform","text":"Fits the components and transforms the text in one step. Source code in tokenwiser/pipeline/_concat.py def fit_transform ( self , X , y = None ): \"\"\" Fits the components and transforms the text in one step. \"\"\" return self . fit ( X , y ) . transform ( X , y )","title":"fit_transform()"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.TextConcat.transform","text":"Transformers the text. Source code in tokenwiser/pipeline/_concat.py def transform ( self , X , y = None ): \"\"\" Transformers the text. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) results = {} for name , tfm in self . transformer_list : results [ name ] = tfm . transform ( X ) return [ \" \" . join ([ results [ n ][ i ] for n in names ]) for i in range ( len ( X ))]","title":"transform()"},{"location":"api/pipeline.html#tokenwiser.pipeline._pipe.make_partial_pipeline","text":"Utility function to generate a PartialPipeline Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.pipeline import make_partial_pipeline from tokenwiser.textprep import HyphenTextPrep , Cleaner tc = make_partial_pipeline ( Cleaner (), HyphenTextPrep ()) data = [ \"dinosaurhead\" , \"another$$ sentence$$\" ] results = tc . fit_partial ( data ) . transform ( data ) expected = [ 'di no saur head' , 'an other sen tence' ] assert results == expected Source code in tokenwiser/pipeline/_pipe.py def make_partial_pipeline ( * steps ): \"\"\" Utility function to generate a `PartialPipeline` Arguments: steps: a collection of text-transformers ```python from tokenwiser.pipeline import make_partial_pipeline from tokenwiser.textprep import HyphenTextPrep, Cleaner tc = make_partial_pipeline(Cleaner(), HyphenTextPrep()) data = [\"dinosaurhead\", \"another$$ sentence$$\"] results = tc.fit_partial(data).transform(data) expected = ['di no saur head', 'an other sen tence'] assert results == expected ``` \"\"\" return PartialPipeline ( _name_estimators ( steps ))","title":"make_partial_pipeline()"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.make_concat","text":"Utility function to generate a TextConcat Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.textprep import HyphenTextPrep , Cleaner from tokenwiser.pipeline import make_concat tc = make_concat ( HyphenTextPrep (), Cleaner ()) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected Source code in tokenwiser/pipeline/_concat.py def make_concat ( * steps ): \"\"\" Utility function to generate a `TextConcat` Arguments: steps: a collection of text-transformers ```python from tokenwiser.textprep import HyphenTextPrep, Cleaner from tokenwiser.pipeline import make_concat tc = make_concat(HyphenTextPrep(), Cleaner()) results = tc.fit_transform([\"dinosaurhead\", \"another$$ sentence$$\"]) expected = ['di no saur head dinosaurhead', 'an other $$ sen tence$$ another sentence'] assert results == expected ``` \"\"\" return TextConcat ( _name_estimators ( steps ))","title":"make_concat()"},{"location":"api/textprep.html","text":"textprep \u00b6 from tokenwiser.textprep import * In the textprep submodule you can find scikit-learn compatbile components that transform text into another type of text. The idea is that this may be combined in interesting ways in CountVectorizers. Cleaner \u00b6 Applies a lowercase and removes non-alphanum. Usage: from tokenwiser.textprep import Cleaner single = Cleaner () . encode_single ( \"$$$5 dollars\" ) assert single == \"5 dollars\" multi = Cleaner () . transform ([ \"$$$5 dollars\" , \"#hashtag!\" ]) assert multi == [ \"5 dollars\" , \"hashtag\" ] HyphenTextPrep \u00b6 Hyphenate the text going in. Usage: from tokenwiser.textprep import HyphenTextPrep multi = HyphenTextPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ] PhoneticTextPrep \u00b6 The ProneticPrep object prepares strings by encoding them phonetically. Parameters: Name Type Description Default kind type of encoding, either \"soundex\" , \" metaphone \" or \"nysiis\" required Usage: import spacy from tokenwiser.textprep import PhoneticTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = PhoneticTextPrep ( kind = \"soundex\" ) . transform ([ \"dinosaurus book\" ]) example2 = PhoneticTextPrep ( kind = \"metaphone\" ) . transform ([ \"dinosaurus book\" ]) example3 = PhoneticTextPrep ( kind = \"nysiis\" ) . transform ([ \"dinosaurus book\" ]) assert example1 [ 0 ] == 'D526 B200' assert example2 [ 0 ] == 'TNSRS BK' assert example3 [ 0 ] == 'DANASAR BAC' YakeTextPrep \u00b6 Remove all text except meaningful key-phrases. Uses yake . Parameters: Name Type Description Default top_n number of key-phrases to select required unique only return unique keywords from the key-phrases required Usage: from tokenwiser.textprep import YakeTextPrep text = [ \"Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning\" ] example = YakeTextPrep ( top_n = 3 , unique = False ) . transform ( text ) assert example [ 0 ] == 'hosts data science acquiring kaggle google is acquiring' SpacyMorphTextPrep \u00b6 Adds morphologic information to tokens in text. Usage: import spacy from tokenwiser.textprep import SpacyMorphTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyMorphTextPrep ( nlp ) . encode_single ( \"quick! duck!\" ) example2 = SpacyMorphTextPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"quick|Degree=Pos !|PunctType=Peri duck|Number=Sing !|PunctType=Peri\" assert example2 == \"hey| look|VerbForm=Inf a|Definite=Ind|PronType=Art duck|Number=Sing\" SpacyPosTextPrep \u00b6 Adds part of speech information per token using spaCy. Parameters: Name Type Description Default model the spaCy model to use required lemma also lemmatize the text required fine_grained use fine grained parts of speech required Usage: import spacy from tokenwiser.textprep import SpacyPosTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyPosTextPrep ( nlp ) . encode_single ( \"we need to duck\" ) example2 = SpacyPosTextPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"we|PRON need|VERB to|PART duck|VERB\" assert example2 == \"hey|INTJ look|VERB a|DET duck|NOUN\" SpacyLemmaTextPrep \u00b6 Turns each token into a lemmatizer version using spaCy. Usage: import spacy from tokenwiser.textprep import SpacyLemmaTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyLemmaTextPrep ( nlp ) . encode_single ( \"we are running\" ) example2 = SpacyLemmaTextPrep ( nlp ) . encode_single ( \"these are dogs\" ) assert example1 == 'we be run' assert example2 == 'these be dog' transform ( self , X , y = None ) \u00b6 Transforms a batch of data. Source code in tokenwiser/textprep/_morph.py def transform ( self , X , y = None ): return [ \" \" . join ([ t . lemma_ for t in d ]) for d in self . model . pipe ( X )]","title":"textprep"},{"location":"api/textprep.html#textprep","text":"from tokenwiser.textprep import * In the textprep submodule you can find scikit-learn compatbile components that transform text into another type of text. The idea is that this may be combined in interesting ways in CountVectorizers.","title":"textprep"},{"location":"api/textprep.html#tokenwiser.textprep._cleaner.Cleaner","text":"Applies a lowercase and removes non-alphanum. Usage: from tokenwiser.textprep import Cleaner single = Cleaner () . encode_single ( \"$$$5 dollars\" ) assert single == \"5 dollars\" multi = Cleaner () . transform ([ \"$$$5 dollars\" , \"#hashtag!\" ]) assert multi == [ \"5 dollars\" , \"hashtag\" ]","title":"Cleaner"},{"location":"api/textprep.html#tokenwiser.textprep._hyphen.HyphenTextPrep","text":"Hyphenate the text going in. Usage: from tokenwiser.textprep import HyphenTextPrep multi = HyphenTextPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ]","title":"HyphenTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._phonetic.PhoneticTextPrep","text":"The ProneticPrep object prepares strings by encoding them phonetically. Parameters: Name Type Description Default kind type of encoding, either \"soundex\" , \" metaphone \" or \"nysiis\" required Usage: import spacy from tokenwiser.textprep import PhoneticTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = PhoneticTextPrep ( kind = \"soundex\" ) . transform ([ \"dinosaurus book\" ]) example2 = PhoneticTextPrep ( kind = \"metaphone\" ) . transform ([ \"dinosaurus book\" ]) example3 = PhoneticTextPrep ( kind = \"nysiis\" ) . transform ([ \"dinosaurus book\" ]) assert example1 [ 0 ] == 'D526 B200' assert example2 [ 0 ] == 'TNSRS BK' assert example3 [ 0 ] == 'DANASAR BAC'","title":"PhoneticTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._yake.YakeTextPrep","text":"Remove all text except meaningful key-phrases. Uses yake . Parameters: Name Type Description Default top_n number of key-phrases to select required unique only return unique keywords from the key-phrases required Usage: from tokenwiser.textprep import YakeTextPrep text = [ \"Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning\" ] example = YakeTextPrep ( top_n = 3 , unique = False ) . transform ( text ) assert example [ 0 ] == 'hosts data science acquiring kaggle google is acquiring'","title":"YakeTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._morph.SpacyMorphTextPrep","text":"Adds morphologic information to tokens in text. Usage: import spacy from tokenwiser.textprep import SpacyMorphTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyMorphTextPrep ( nlp ) . encode_single ( \"quick! duck!\" ) example2 = SpacyMorphTextPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"quick|Degree=Pos !|PunctType=Peri duck|Number=Sing !|PunctType=Peri\" assert example2 == \"hey| look|VerbForm=Inf a|Definite=Ind|PronType=Art duck|Number=Sing\"","title":"SpacyMorphTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._morph.SpacyPosTextPrep","text":"Adds part of speech information per token using spaCy. Parameters: Name Type Description Default model the spaCy model to use required lemma also lemmatize the text required fine_grained use fine grained parts of speech required Usage: import spacy from tokenwiser.textprep import SpacyPosTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyPosTextPrep ( nlp ) . encode_single ( \"we need to duck\" ) example2 = SpacyPosTextPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"we|PRON need|VERB to|PART duck|VERB\" assert example2 == \"hey|INTJ look|VERB a|DET duck|NOUN\"","title":"SpacyPosTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._morph.SpacyLemmaTextPrep","text":"Turns each token into a lemmatizer version using spaCy. Usage: import spacy from tokenwiser.textprep import SpacyLemmaTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyLemmaTextPrep ( nlp ) . encode_single ( \"we are running\" ) example2 = SpacyLemmaTextPrep ( nlp ) . encode_single ( \"these are dogs\" ) assert example1 == 'we be run' assert example2 == 'these be dog'","title":"SpacyLemmaTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._morph.SpacyLemmaTextPrep.transform","text":"Transforms a batch of data. Source code in tokenwiser/textprep/_morph.py def transform ( self , X , y = None ): return [ \" \" . join ([ t . lemma_ for t in d ]) for d in self . model . pipe ( X )]","title":"transform()"}]}