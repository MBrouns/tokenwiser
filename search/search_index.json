{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"tokenwiser Bag of, not words, but tricks! Goal \u00b6 We noticed that a lot of benchmarks relied on heavy-weight tools while they did not check if something more lightweight would also work. Maybe if we just apply some simple tricks on our tokens we won't need massive language models. The goal of this package is to contribute tricks to keep your NLP pipelines simple. These tricks are made available for spaCy, scikit-learn and vowpal wabbit. If you're looking for a tool that can add pretrained language models to scikit-learn pipelines as a benchmark you'll want to explore another tool: whatlies . Features \u00b6 Scikit-Learn Tools \u00b6 The following submodules contain features that might be useful. .textprep : Contains string pre-processing tools for scikit-learn. Takes a string in and pushes a string out. .pipeline : Contains extra pipeline components for scikit-learn to make it easier to work with strings and partial models. SpaCy Tools \u00b6 .component : Contains spaCy compatible components that might be added as a pipeline step. .extension : Contains spaCy compatible extensions that might be added manually.","title":"Home"},{"location":"index.html#goal","text":"We noticed that a lot of benchmarks relied on heavy-weight tools while they did not check if something more lightweight would also work. Maybe if we just apply some simple tricks on our tokens we won't need massive language models. The goal of this package is to contribute tricks to keep your NLP pipelines simple. These tricks are made available for spaCy, scikit-learn and vowpal wabbit. If you're looking for a tool that can add pretrained language models to scikit-learn pipelines as a benchmark you'll want to explore another tool: whatlies .","title":"Goal"},{"location":"index.html#features","text":"","title":"Features"},{"location":"index.html#scikit-learn-tools","text":"The following submodules contain features that might be useful. .textprep : Contains string pre-processing tools for scikit-learn. Takes a string in and pushes a string out. .pipeline : Contains extra pipeline components for scikit-learn to make it easier to work with strings and partial models.","title":"Scikit-Learn Tools"},{"location":"index.html#spacy-tools","text":".component : Contains spaCy compatible components that might be added as a pipeline step. .extension : Contains spaCy compatible extensions that might be added manually.","title":"SpaCy Tools"},{"location":"faq.html","text":"Why can't I use normal Pipeline objects with the spaCy API? \u00b6 Scikit-Learn assumes that data is trained via .fit ( X , y ). predict ( X ) . This is great when you've got a dataset fully in memory but it's not so great when your dataset is too big to fit in one go. This is a main reason why spaCy has an .update () API for their trainable pipeline components. It's similar to .partial_fit ( X ) in scikit-learn. You wouldn't train on a single batch of data. Instead you would iteratively train on subsets of the dataset. A big downside of the Pipeline API is that it cannot use .partial_fit ( X ) . Even if all the components on the inside are compatible, it forces you to use .fit ( X ) . That is why this library offers a PartialPipeline . It only allows for components that have .partial_fit implemented and it's these pipelines that can also comply with spaCy's .update () API. Note that all scikit-learn components offered by this library are compatible with the PartialPipeline . This includes everything from the tokeniser.textprep submodule. Can I train spaCy with scikit-learn from Jupyter? \u00b6 It's not our favorite way of doing things, but nobody is stopping you. import spacy from spacy import registry from spacy.training import Example from spacy.language import Language from tokenwiser.pipeline import PartialPipeline from tokenwiser.model.sklearnmod import SklearnCat from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier @Language . factory ( \"custom-sklearn-cat\" ) def make_sklearn_cat ( nlp , name , sklearn_model , label , classes ): return SklearnCat ( nlp , name , sklearn_model , label , classes ) @registry . architectures ( \"sklearn_model_basic_sgd.v1\" ) def make_sklearn_cat_basic_sgd (): \"\"\"This creates a *partial* pipeline. We can't use a standard pipeline from scikit-learn.\"\"\" return PartialPipeline ([( \"hash\" , HashingVectorizer ()), ( \"lr\" , SGDClassifier ( loss = \"log\" ))]) nlp = spacy . load ( \"en_core_web_sm\" ) config = { \"sklearn_model\" : \"@sklearn_model_basic_sgd.v1\" , \"label\" : \"pos\" , \"classes\" : [ \"pos\" , \"neg\" ] } nlp . add_pipe ( \"custom-sklearn-cat\" , config = config ) texts = [ \"you are a nice person\" , \"this is a great movie\" , \"i do not like cofee\" , \"i hate tea\" ] labels = [ \"pos\" , \"pos\" , \"neg\" , \"neg\" ] # This is the training loop just for out categorizer model. with nlp . select_pipes ( enable = \"custom-sklearn-cat\" ): optimizer = nlp . resume_training () for loop in range ( 10 ): for t , lab in zip ( texts , labels ): doc = nlp . make_doc ( t ) example = Example . from_dict ( doc , { \"cats\" : { \"pos\" : lab }}) nlp . update ([ example ], sgd = optimizer ) nlp ( \"you are a nice person\" ) . cats # {'pos': 0.9979167909733176} nlp ( \"coffee i do not like\" ) . cats # {'neg': 0.990049724779963}","title":"FAQ"},{"location":"faq.html#why-cant-i-use-normal-pipeline-objects-with-the-spacy-api","text":"Scikit-Learn assumes that data is trained via .fit ( X , y ). predict ( X ) . This is great when you've got a dataset fully in memory but it's not so great when your dataset is too big to fit in one go. This is a main reason why spaCy has an .update () API for their trainable pipeline components. It's similar to .partial_fit ( X ) in scikit-learn. You wouldn't train on a single batch of data. Instead you would iteratively train on subsets of the dataset. A big downside of the Pipeline API is that it cannot use .partial_fit ( X ) . Even if all the components on the inside are compatible, it forces you to use .fit ( X ) . That is why this library offers a PartialPipeline . It only allows for components that have .partial_fit implemented and it's these pipelines that can also comply with spaCy's .update () API. Note that all scikit-learn components offered by this library are compatible with the PartialPipeline . This includes everything from the tokeniser.textprep submodule.","title":"Why can't I use normal Pipeline objects with the spaCy API?"},{"location":"faq.html#can-i-train-spacy-with-scikit-learn-from-jupyter","text":"It's not our favorite way of doing things, but nobody is stopping you. import spacy from spacy import registry from spacy.training import Example from spacy.language import Language from tokenwiser.pipeline import PartialPipeline from tokenwiser.model.sklearnmod import SklearnCat from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier @Language . factory ( \"custom-sklearn-cat\" ) def make_sklearn_cat ( nlp , name , sklearn_model , label , classes ): return SklearnCat ( nlp , name , sklearn_model , label , classes ) @registry . architectures ( \"sklearn_model_basic_sgd.v1\" ) def make_sklearn_cat_basic_sgd (): \"\"\"This creates a *partial* pipeline. We can't use a standard pipeline from scikit-learn.\"\"\" return PartialPipeline ([( \"hash\" , HashingVectorizer ()), ( \"lr\" , SGDClassifier ( loss = \"log\" ))]) nlp = spacy . load ( \"en_core_web_sm\" ) config = { \"sklearn_model\" : \"@sklearn_model_basic_sgd.v1\" , \"label\" : \"pos\" , \"classes\" : [ \"pos\" , \"neg\" ] } nlp . add_pipe ( \"custom-sklearn-cat\" , config = config ) texts = [ \"you are a nice person\" , \"this is a great movie\" , \"i do not like cofee\" , \"i hate tea\" ] labels = [ \"pos\" , \"pos\" , \"neg\" , \"neg\" ] # This is the training loop just for out categorizer model. with nlp . select_pipes ( enable = \"custom-sklearn-cat\" ): optimizer = nlp . resume_training () for loop in range ( 10 ): for t , lab in zip ( texts , labels ): doc = nlp . make_doc ( t ) example = Example . from_dict ( doc , { \"cats\" : { \"pos\" : lab }}) nlp . update ([ example ], sgd = optimizer ) nlp ( \"you are a nice person\" ) . cats # {'pos': 0.9979167909733176} nlp ( \"coffee i do not like\" ) . cats # {'neg': 0.990049724779963}","title":"Can I train spaCy with scikit-learn from Jupyter?"},{"location":"api/component.html","text":"component \u00b6 from tokenwiser.component import * In the component submodule you can find spaCy compatible components. attach_sklearn_categoriser ( nlp , pipe_name , estimator ) \u00b6 This function will attach a scikit-learn compatible estimator to the pipeline which will feed predictions to the .cats property. This is useful if you're interesting in added a pre-trained sklearn model to the pipeline. This is not useful if you're interested in training a new model via spaCy, check out the tokenwiser.model submodule for that. Usage: import spacy from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.component import attach_sklearn_categoriser X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = [ \"pos\" , \"pos\" , \"pos\" , \"neg\" , \"neg\" , \"neg\" ] # Note that we're training a pipeline here via a single-batch `.fit()` method pipe = make_pipeline ( CountVectorizer (), LogisticRegression ()) . fit ( X , y ) nlp = spacy . load ( \"en_core_web_sm\" ) # This is where we attach our pre-trained model as a pipeline step. attach_sklearn_categoriser ( nlp , pipe_name = \"silly_sentiment\" , estimator = pipe ) assert nlp . pipe_names [ - 1 ] == \"silly_sentiment\" assert nlp ( \"this post i really like\" ) . cats [ \"pos\" ] > 0.5 Source code in tokenwiser/component/_sklearn.py def attach_sklearn_categoriser ( nlp , pipe_name , estimator ): \"\"\" This function will attach a scikit-learn compatible estimator to the pipeline which will feed predictions to the `.cats` property. This is useful if you're interesting in added a pre-trained sklearn model to the pipeline. This is **not** useful if you're interested in training a new model via spaCy, check out the `tokenwiser.model` submodule for that. Usage: ```python import spacy from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.component import attach_sklearn_categoriser X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = [\"pos\", \"pos\", \"pos\", \"neg\", \"neg\", \"neg\"] # Note that we're training a pipeline here via a single-batch `.fit()` method pipe = make_pipeline(CountVectorizer(), LogisticRegression()).fit(X, y) nlp = spacy.load(\"en_core_web_sm\") # This is where we attach our pre-trained model as a pipeline step. attach_sklearn_categoriser(nlp, pipe_name=\"silly_sentiment\", estimator=pipe) assert nlp.pipe_names[-1] == \"silly_sentiment\" assert nlp(\"this post i really like\").cats[\"pos\"] > 0.5 ``` \"\"\" @Language . component ( pipe_name ) def my_component ( doc ): pred = estimator . predict ([ doc . text ])[ 0 ] proba = estimator . predict_proba ([ doc . text ]) . max () doc . cats [ pred ] = proba return doc nlp . add_pipe ( pipe_name )","title":"component"},{"location":"api/component.html#component","text":"from tokenwiser.component import * In the component submodule you can find spaCy compatible components.","title":"component"},{"location":"api/component.html#tokenwiser.component._sklearn.attach_sklearn_categoriser","text":"This function will attach a scikit-learn compatible estimator to the pipeline which will feed predictions to the .cats property. This is useful if you're interesting in added a pre-trained sklearn model to the pipeline. This is not useful if you're interested in training a new model via spaCy, check out the tokenwiser.model submodule for that. Usage: import spacy from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.component import attach_sklearn_categoriser X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = [ \"pos\" , \"pos\" , \"pos\" , \"neg\" , \"neg\" , \"neg\" ] # Note that we're training a pipeline here via a single-batch `.fit()` method pipe = make_pipeline ( CountVectorizer (), LogisticRegression ()) . fit ( X , y ) nlp = spacy . load ( \"en_core_web_sm\" ) # This is where we attach our pre-trained model as a pipeline step. attach_sklearn_categoriser ( nlp , pipe_name = \"silly_sentiment\" , estimator = pipe ) assert nlp . pipe_names [ - 1 ] == \"silly_sentiment\" assert nlp ( \"this post i really like\" ) . cats [ \"pos\" ] > 0.5 Source code in tokenwiser/component/_sklearn.py def attach_sklearn_categoriser ( nlp , pipe_name , estimator ): \"\"\" This function will attach a scikit-learn compatible estimator to the pipeline which will feed predictions to the `.cats` property. This is useful if you're interesting in added a pre-trained sklearn model to the pipeline. This is **not** useful if you're interested in training a new model via spaCy, check out the `tokenwiser.model` submodule for that. Usage: ```python import spacy from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.component import attach_sklearn_categoriser X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = [\"pos\", \"pos\", \"pos\", \"neg\", \"neg\", \"neg\"] # Note that we're training a pipeline here via a single-batch `.fit()` method pipe = make_pipeline(CountVectorizer(), LogisticRegression()).fit(X, y) nlp = spacy.load(\"en_core_web_sm\") # This is where we attach our pre-trained model as a pipeline step. attach_sklearn_categoriser(nlp, pipe_name=\"silly_sentiment\", estimator=pipe) assert nlp.pipe_names[-1] == \"silly_sentiment\" assert nlp(\"this post i really like\").cats[\"pos\"] > 0.5 ``` \"\"\" @Language . component ( pipe_name ) def my_component ( doc ): pred = estimator . predict ([ doc . text ])[ 0 ] proba = estimator . predict_proba ([ doc . text ]) . max () doc . cats [ pred ] = proba return doc nlp . add_pipe ( pipe_name )","title":"attach_sklearn_categoriser()"},{"location":"api/extension.html","text":"extension \u00b6 from tokenwiser.extension import * In the extension submodule you can find spaCy compatible extensions. attach_hyphen_extension () \u00b6 This function will attach an extension ._.hyphen to the Token s. import spacy from tokenwiser.extension import attach_hyphen_extension nlp = spacy . load ( \"en_core_web_sm\" ) # Attach the Hyphen extensions. attach_hyphen_extension () # Now you can query hyphens on the tokens. doc = nlp ( \"this is a dinosaurhead\" ) tok = doc [ - 1 ] assert tok . _ . hyphen == [ \"di\" , \"no\" , \"saur\" , \"head\" ] Source code in tokenwiser/extension/_extension.py def attach_hyphen_extension (): \"\"\" This function will attach an extension `._.hyphen` to the `Token`s. ```python import spacy from tokenwiser.extension import attach_hyphen_extension nlp = spacy.load(\"en_core_web_sm\") # Attach the Hyphen extensions. attach_hyphen_extension() # Now you can query hyphens on the tokens. doc = nlp(\"this is a dinosaurhead\") tok = doc[-1] assert tok._.hyphen == [\"di\", \"no\", \"saur\", \"head\"] ``` \"\"\" Token . set_extension ( \"hyphen\" , getter = lambda t : HyphenTextPrep () . encode_single ( t . text ) . split ( \" \" ), force = True , ) sklearn_method ( estimator ) \u00b6 A helper to turn a scikit-learn estimator into a spaCy extension. Just in case you really wanted to do it manually. import spacy from spacy.tokens import Doc from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.extension import sklearn_method X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = [ \"pos\" , \"pos\" , \"pos\" , \"neg\" , \"neg\" , \"neg\" ] # First we train a (silly) model. mod = make_pipeline ( CountVectorizer (), LogisticRegression ()) . fit ( X , y ) # This is where we attach the scikit-learn model to spaCy as a method extension. Doc . set_extension ( \"sillysent_method\" , method = sklearn_method ( mod )) # This is where we attach the scikit-learn model to spaCy as a property extension. Doc . set_extension ( \"sillysent_prop\" , getter = sklearn_method ( mod )) # Demo nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( \"thank you, really nice\" ) doc . _ . sillysent_method () # {\"neg\": 0.4446964938410244, \"pos: 0.5553035061589756} doc . _ . sillysent_prop # {\"neg: 0.4446964938410244, \"pos\": 0.5553035061589756} Source code in tokenwiser/extension/_extension.py def sklearn_method ( estimator ): \"\"\" A helper to turn a scikit-learn estimator into a spaCy extension. Just in case you *really* wanted to do it manually. ```python import spacy from spacy.tokens import Doc from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.extension import sklearn_method X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = [\"pos\", \"pos\", \"pos\", \"neg\", \"neg\", \"neg\"] # First we train a (silly) model. mod = make_pipeline(CountVectorizer(), LogisticRegression()).fit(X, y) # This is where we attach the scikit-learn model to spaCy as a method extension. Doc.set_extension(\"sillysent_method\", method=sklearn_method(mod)) # This is where we attach the scikit-learn model to spaCy as a property extension. Doc.set_extension(\"sillysent_prop\", getter=sklearn_method(mod)) # Demo nlp = spacy.load(\"en_core_web_sm\") doc = nlp(\"thank you, really nice\") doc._.sillysent_method() # {\"neg\": 0.4446964938410244, \"pos: 0.5553035061589756} doc._.sillysent_prop # {\"neg: 0.4446964938410244, \"pos\": 0.5553035061589756} ``` \"\"\" def method ( doc ): proba = estimator . predict_proba ([ doc . text ])[ 0 ] return { c : p for c , p in zip ( estimator . classes_ , proba )} return method","title":"extension"},{"location":"api/extension.html#extension","text":"from tokenwiser.extension import * In the extension submodule you can find spaCy compatible extensions.","title":"extension"},{"location":"api/extension.html#tokenwiser.extension._extension.attach_hyphen_extension","text":"This function will attach an extension ._.hyphen to the Token s. import spacy from tokenwiser.extension import attach_hyphen_extension nlp = spacy . load ( \"en_core_web_sm\" ) # Attach the Hyphen extensions. attach_hyphen_extension () # Now you can query hyphens on the tokens. doc = nlp ( \"this is a dinosaurhead\" ) tok = doc [ - 1 ] assert tok . _ . hyphen == [ \"di\" , \"no\" , \"saur\" , \"head\" ] Source code in tokenwiser/extension/_extension.py def attach_hyphen_extension (): \"\"\" This function will attach an extension `._.hyphen` to the `Token`s. ```python import spacy from tokenwiser.extension import attach_hyphen_extension nlp = spacy.load(\"en_core_web_sm\") # Attach the Hyphen extensions. attach_hyphen_extension() # Now you can query hyphens on the tokens. doc = nlp(\"this is a dinosaurhead\") tok = doc[-1] assert tok._.hyphen == [\"di\", \"no\", \"saur\", \"head\"] ``` \"\"\" Token . set_extension ( \"hyphen\" , getter = lambda t : HyphenTextPrep () . encode_single ( t . text ) . split ( \" \" ), force = True , )","title":"attach_hyphen_extension()"},{"location":"api/extension.html#tokenwiser.extension._extension.sklearn_method","text":"A helper to turn a scikit-learn estimator into a spaCy extension. Just in case you really wanted to do it manually. import spacy from spacy.tokens import Doc from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.extension import sklearn_method X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = [ \"pos\" , \"pos\" , \"pos\" , \"neg\" , \"neg\" , \"neg\" ] # First we train a (silly) model. mod = make_pipeline ( CountVectorizer (), LogisticRegression ()) . fit ( X , y ) # This is where we attach the scikit-learn model to spaCy as a method extension. Doc . set_extension ( \"sillysent_method\" , method = sklearn_method ( mod )) # This is where we attach the scikit-learn model to spaCy as a property extension. Doc . set_extension ( \"sillysent_prop\" , getter = sklearn_method ( mod )) # Demo nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( \"thank you, really nice\" ) doc . _ . sillysent_method () # {\"neg\": 0.4446964938410244, \"pos: 0.5553035061589756} doc . _ . sillysent_prop # {\"neg: 0.4446964938410244, \"pos\": 0.5553035061589756} Source code in tokenwiser/extension/_extension.py def sklearn_method ( estimator ): \"\"\" A helper to turn a scikit-learn estimator into a spaCy extension. Just in case you *really* wanted to do it manually. ```python import spacy from spacy.tokens import Doc from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.extension import sklearn_method X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = [\"pos\", \"pos\", \"pos\", \"neg\", \"neg\", \"neg\"] # First we train a (silly) model. mod = make_pipeline(CountVectorizer(), LogisticRegression()).fit(X, y) # This is where we attach the scikit-learn model to spaCy as a method extension. Doc.set_extension(\"sillysent_method\", method=sklearn_method(mod)) # This is where we attach the scikit-learn model to spaCy as a property extension. Doc.set_extension(\"sillysent_prop\", getter=sklearn_method(mod)) # Demo nlp = spacy.load(\"en_core_web_sm\") doc = nlp(\"thank you, really nice\") doc._.sillysent_method() # {\"neg\": 0.4446964938410244, \"pos: 0.5553035061589756} doc._.sillysent_prop # {\"neg: 0.4446964938410244, \"pos\": 0.5553035061589756} ``` \"\"\" def method ( doc ): proba = estimator . predict_proba ([ doc . text ])[ 0 ] return { c : p for c , p in zip ( estimator . classes_ , proba )} return method","title":"sklearn_method()"},{"location":"api/model.html","text":"model \u00b6 from tokenwiser.model import * In the model submodule you can find scikit-learn pipelines that are trainable via spaCy. These pipelines apply the .partial_fit (). predict () -design which makes them compliant with the spacy train command. SklearnCat \u00b6 This is a spaCy pipeline component object that can train specific scikit-learn pipelines. This allows you to run a simple benchmark via spaCy on simple text-based scikit-learn models. One should not expect these models to have state of the art accuracy. But they should have \"pretty good\" accuracy while being substantially faster to train than most deep-learning based models. The intended use-case for these models is to offer a base benchmark. If these models perform well one your task, it's an indication that you're in luck and that you've got a simple task that doesn't require state of the art models.","title":"`model`"},{"location":"api/model.html#model","text":"from tokenwiser.model import * In the model submodule you can find scikit-learn pipelines that are trainable via spaCy. These pipelines apply the .partial_fit (). predict () -design which makes them compliant with the spacy train command.","title":"model"},{"location":"api/model.html#tokenwiser.model.sklearnmod.SklearnCat","text":"This is a spaCy pipeline component object that can train specific scikit-learn pipelines. This allows you to run a simple benchmark via spaCy on simple text-based scikit-learn models. One should not expect these models to have state of the art accuracy. But they should have \"pretty good\" accuracy while being substantially faster to train than most deep-learning based models. The intended use-case for these models is to offer a base benchmark. If these models perform well one your task, it's an indication that you're in luck and that you've got a simple task that doesn't require state of the art models.","title":"SklearnCat"},{"location":"api/pipeline.html","text":"pipeline \u00b6 from tokenwiser.pipeline import * In the pipeline submodule you can find scikit-learn compatbile pipelines that extend the standard behavior. PartialPipeline \u00b6 Utility function to generate a PartialPipeline Parameters: Name Type Description Default steps a collection of text-transformers required from tokenwiser.pipeline import PartialPipeline from tokenwiser.textprep import HyphenTextPrep , Cleaner tc = PartialPipeline ([( 'clean' , Cleaner ()), ( 'hyp' , HyphenTextPrep ())]) data = [ \"dinosaurhead\" , \"another$$ sentence$$\" ] results = tc . partial_fit ( data ) . transform ( data ) expected = [ 'di no saur head' , 'an other sen tence' ] assert results == expected partial_fit ( self , X , y = None , classes = None , ** kwargs ) \u00b6 Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_pipe.py def partial_fit ( self , X , y = None , classes = None , ** kwargs ): \"\"\" Fits the components, but allow for batches. \"\"\" for name , step in self . steps : if not hasattr ( step , \"partial_fit\" ): raise ValueError ( f \"Step { name } is a { step } which does not have `.partial_fit` implemented.\" ) for name , step in self . steps : if hasattr ( step , 'predict' ): step . partial_fit ( X , y , classes = classes , ** kwargs ) else : step . partial_fit ( X , y ) if hasattr ( step , 'transform' ): X = step . transform ( X ) return self TextConcat \u00b6 A component like FeatureUnion but this also concatenates the text. Parameters: Name Type Description Default transformer_list list of (name, text-transformer)-tuples required Examples: from tokenwiser.textprep import HyphenTextPrep , Cleaner from tokenwiser.pipeline import TextConcat tc = TextConcat ([( \"hyp\" , HyphenTextPrep ()), ( \"clean\" , Cleaner ())]) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected fit ( self , X , y = None ) \u00b6 Fits the components in a single batch. Source code in tokenwiser/pipeline/_concat.py def fit ( self , X , y = None ): \"\"\" Fits the components in a single batch. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) return self fit_transform ( self , X , y = None ) \u00b6 Fits the components and transforms the text in one step. Source code in tokenwiser/pipeline/_concat.py def fit_transform ( self , X , y = None ): \"\"\" Fits the components and transforms the text in one step. \"\"\" return self . fit ( X , y ) . transform ( X , y ) partial_fit ( self , X , y = None ) \u00b6 Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_concat.py def partial_fit ( self , X , y = None ): \"\"\" Fits the components, but allow for batches. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) return self transform ( self , X , y = None ) \u00b6 Transformers the text. Source code in tokenwiser/pipeline/_concat.py def transform ( self , X , y = None ): \"\"\" Transformers the text. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) results = {} for name , tfm in self . transformer_list : results [ name ] = tfm . transform ( X ) return [ \" \" . join ([ results [ n ][ i ] for n in names ]) for i in range ( len ( X ))] make_partial_pipeline ( * steps ) \u00b6 Utility function to generate a PartialPipeline Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.pipeline import make_partial_pipeline from tokenwiser.textprep import HyphenTextPrep , Cleaner tc = make_partial_pipeline ( Cleaner (), HyphenTextPrep ()) data = [ \"dinosaurhead\" , \"another$$ sentence$$\" ] results = tc . partial_fit ( data ) . transform ( data ) expected = [ 'di no saur head' , 'an other sen tence' ] assert results == expected Source code in tokenwiser/pipeline/_pipe.py def make_partial_pipeline ( * steps ): \"\"\" Utility function to generate a `PartialPipeline` Arguments: steps: a collection of text-transformers ```python from tokenwiser.pipeline import make_partial_pipeline from tokenwiser.textprep import HyphenTextPrep, Cleaner tc = make_partial_pipeline(Cleaner(), HyphenTextPrep()) data = [\"dinosaurhead\", \"another$$ sentence$$\"] results = tc.partial_fit(data).transform(data) expected = ['di no saur head', 'an other sen tence'] assert results == expected ``` \"\"\" return PartialPipeline ( _name_estimators ( steps )) make_concat ( * steps ) \u00b6 Utility function to generate a TextConcat Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.textprep import HyphenTextPrep , Cleaner from tokenwiser.pipeline import make_concat tc = make_concat ( HyphenTextPrep (), Cleaner ()) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected Source code in tokenwiser/pipeline/_concat.py def make_concat ( * steps ): \"\"\" Utility function to generate a `TextConcat` Arguments: steps: a collection of text-transformers ```python from tokenwiser.textprep import HyphenTextPrep, Cleaner from tokenwiser.pipeline import make_concat tc = make_concat(HyphenTextPrep(), Cleaner()) results = tc.fit_transform([\"dinosaurhead\", \"another$$ sentence$$\"]) expected = ['di no saur head dinosaurhead', 'an other $$ sen tence$$ another sentence'] assert results == expected ``` \"\"\" return TextConcat ( _name_estimators ( steps ))","title":"pipeline"},{"location":"api/pipeline.html#pipeline","text":"from tokenwiser.pipeline import * In the pipeline submodule you can find scikit-learn compatbile pipelines that extend the standard behavior.","title":"pipeline"},{"location":"api/pipeline.html#tokenwiser.pipeline._pipe.PartialPipeline","text":"Utility function to generate a PartialPipeline Parameters: Name Type Description Default steps a collection of text-transformers required from tokenwiser.pipeline import PartialPipeline from tokenwiser.textprep import HyphenTextPrep , Cleaner tc = PartialPipeline ([( 'clean' , Cleaner ()), ( 'hyp' , HyphenTextPrep ())]) data = [ \"dinosaurhead\" , \"another$$ sentence$$\" ] results = tc . partial_fit ( data ) . transform ( data ) expected = [ 'di no saur head' , 'an other sen tence' ] assert results == expected","title":"PartialPipeline"},{"location":"api/pipeline.html#tokenwiser.pipeline._pipe.PartialPipeline.partial_fit","text":"Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_pipe.py def partial_fit ( self , X , y = None , classes = None , ** kwargs ): \"\"\" Fits the components, but allow for batches. \"\"\" for name , step in self . steps : if not hasattr ( step , \"partial_fit\" ): raise ValueError ( f \"Step { name } is a { step } which does not have `.partial_fit` implemented.\" ) for name , step in self . steps : if hasattr ( step , 'predict' ): step . partial_fit ( X , y , classes = classes , ** kwargs ) else : step . partial_fit ( X , y ) if hasattr ( step , 'transform' ): X = step . transform ( X ) return self","title":"partial_fit()"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.TextConcat","text":"A component like FeatureUnion but this also concatenates the text. Parameters: Name Type Description Default transformer_list list of (name, text-transformer)-tuples required Examples: from tokenwiser.textprep import HyphenTextPrep , Cleaner from tokenwiser.pipeline import TextConcat tc = TextConcat ([( \"hyp\" , HyphenTextPrep ()), ( \"clean\" , Cleaner ())]) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected","title":"TextConcat"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.TextConcat.fit","text":"Fits the components in a single batch. Source code in tokenwiser/pipeline/_concat.py def fit ( self , X , y = None ): \"\"\" Fits the components in a single batch. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) return self","title":"fit()"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.TextConcat.fit_transform","text":"Fits the components and transforms the text in one step. Source code in tokenwiser/pipeline/_concat.py def fit_transform ( self , X , y = None ): \"\"\" Fits the components and transforms the text in one step. \"\"\" return self . fit ( X , y ) . transform ( X , y )","title":"fit_transform()"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.TextConcat.partial_fit","text":"Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_concat.py def partial_fit ( self , X , y = None ): \"\"\" Fits the components, but allow for batches. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) return self","title":"partial_fit()"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.TextConcat.transform","text":"Transformers the text. Source code in tokenwiser/pipeline/_concat.py def transform ( self , X , y = None ): \"\"\" Transformers the text. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) results = {} for name , tfm in self . transformer_list : results [ name ] = tfm . transform ( X ) return [ \" \" . join ([ results [ n ][ i ] for n in names ]) for i in range ( len ( X ))]","title":"transform()"},{"location":"api/pipeline.html#tokenwiser.pipeline._pipe.make_partial_pipeline","text":"Utility function to generate a PartialPipeline Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.pipeline import make_partial_pipeline from tokenwiser.textprep import HyphenTextPrep , Cleaner tc = make_partial_pipeline ( Cleaner (), HyphenTextPrep ()) data = [ \"dinosaurhead\" , \"another$$ sentence$$\" ] results = tc . partial_fit ( data ) . transform ( data ) expected = [ 'di no saur head' , 'an other sen tence' ] assert results == expected Source code in tokenwiser/pipeline/_pipe.py def make_partial_pipeline ( * steps ): \"\"\" Utility function to generate a `PartialPipeline` Arguments: steps: a collection of text-transformers ```python from tokenwiser.pipeline import make_partial_pipeline from tokenwiser.textprep import HyphenTextPrep, Cleaner tc = make_partial_pipeline(Cleaner(), HyphenTextPrep()) data = [\"dinosaurhead\", \"another$$ sentence$$\"] results = tc.partial_fit(data).transform(data) expected = ['di no saur head', 'an other sen tence'] assert results == expected ``` \"\"\" return PartialPipeline ( _name_estimators ( steps ))","title":"make_partial_pipeline()"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.make_concat","text":"Utility function to generate a TextConcat Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.textprep import HyphenTextPrep , Cleaner from tokenwiser.pipeline import make_concat tc = make_concat ( HyphenTextPrep (), Cleaner ()) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected Source code in tokenwiser/pipeline/_concat.py def make_concat ( * steps ): \"\"\" Utility function to generate a `TextConcat` Arguments: steps: a collection of text-transformers ```python from tokenwiser.textprep import HyphenTextPrep, Cleaner from tokenwiser.pipeline import make_concat tc = make_concat(HyphenTextPrep(), Cleaner()) results = tc.fit_transform([\"dinosaurhead\", \"another$$ sentence$$\"]) expected = ['di no saur head dinosaurhead', 'an other $$ sen tence$$ another sentence'] assert results == expected ``` \"\"\" return TextConcat ( _name_estimators ( steps ))","title":"make_concat()"},{"location":"api/textprep.html","text":"textprep \u00b6 from tokenwiser.textprep import * In the textprep submodule you can find scikit-learn compatbile components that transform text into another type of text. The idea is that this may be combined in interesting ways in CountVectorizers. Cleaner \u00b6 Applies a lowercase and removes non-alphanum. Usage: from tokenwiser.textprep import Cleaner single = Cleaner () . encode_single ( \"$$$5 dollars\" ) assert single == \"5 dollars\" multi = Cleaner () . transform ([ \"$$$5 dollars\" , \"#hashtag!\" ]) assert multi == [ \"5 dollars\" , \"hashtag\" ] Identity \u00b6 Keeps the text as is. Can be used as a placeholder in a pipeline. Usage: from tokenwiser.textprep import Identity text = [ \"hello\" , \"world\" ] example = Identity () . transform ( text ) assert example == [ \"hello\" , \"world\" ] The main use-case is as a placeholder. from tokenwiser.pipeline import make_concat from sklearn.pipeline import make_pipeline , make_union from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), ) fit ( self , X , y = None ) \u00b6 Fits the TextPrep step. Considered a no-op. Source code in tokenwiser/textprep/_identity.py def fit ( self , X , y = None ): return self partial_fit ( self , X , y = None ) \u00b6 Partially fits the TextPrep step. Considered a no-op. Source code in tokenwiser/textprep/_identity.py def partial_fit ( self , X , y = None ): return self HyphenTextPrep \u00b6 Hyphenate the text going in. Usage: from tokenwiser.textprep import HyphenTextPrep multi = HyphenTextPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ] PhoneticTextPrep \u00b6 The ProneticPrep object prepares strings by encoding them phonetically. Parameters: Name Type Description Default kind type of encoding, either \"soundex\" , \" metaphone \" or \"nysiis\" required Usage: import spacy from tokenwiser.textprep import PhoneticTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = PhoneticTextPrep ( kind = \"soundex\" ) . transform ([ \"dinosaurus book\" ]) example2 = PhoneticTextPrep ( kind = \"metaphone\" ) . transform ([ \"dinosaurus book\" ]) example3 = PhoneticTextPrep ( kind = \"nysiis\" ) . transform ([ \"dinosaurus book\" ]) assert example1 [ 0 ] == 'D526 B200' assert example2 [ 0 ] == 'TNSRS BK' assert example3 [ 0 ] == 'DANASAR BAC' YakeTextPrep \u00b6 Remove all text except meaningful key-phrases. Uses yake . Parameters: Name Type Description Default top_n number of key-phrases to select required unique only return unique keywords from the key-phrases required Usage: from tokenwiser.textprep import YakeTextPrep text = [ \"Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning\" ] example = YakeTextPrep ( top_n = 3 , unique = False ) . transform ( text ) assert example [ 0 ] == 'hosts data science acquiring kaggle google is acquiring' SpacyMorphTextPrep \u00b6 Adds morphologic information to tokens in text. Usage: import spacy from tokenwiser.textprep import SpacyMorphTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyMorphTextPrep ( nlp ) . encode_single ( \"quick! duck!\" ) example2 = SpacyMorphTextPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"quick|Degree=Pos !|PunctType=Peri duck|Number=Sing !|PunctType=Peri\" assert example2 == \"hey| look|VerbForm=Inf a|Definite=Ind|PronType=Art duck|Number=Sing\" SpacyPosTextPrep \u00b6 Adds part of speech information per token using spaCy. Parameters: Name Type Description Default model the spaCy model to use required lemma also lemmatize the text required fine_grained use fine grained parts of speech required Usage: import spacy from tokenwiser.textprep import SpacyPosTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyPosTextPrep ( nlp ) . encode_single ( \"we need to duck\" ) example2 = SpacyPosTextPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"we|PRON need|VERB to|PART duck|VERB\" assert example2 == \"hey|INTJ look|VERB a|DET duck|NOUN\" SpacyLemmaTextPrep \u00b6 Turns each token into a lemmatizer version using spaCy. Usage: import spacy from tokenwiser.textprep import SpacyLemmaTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyLemmaTextPrep ( nlp ) . encode_single ( \"we are running\" ) example2 = SpacyLemmaTextPrep ( nlp ) . encode_single ( \"these are dogs\" ) assert example1 == 'we be run' assert example2 == 'these be dog'","title":"textprep"},{"location":"api/textprep.html#textprep","text":"from tokenwiser.textprep import * In the textprep submodule you can find scikit-learn compatbile components that transform text into another type of text. The idea is that this may be combined in interesting ways in CountVectorizers.","title":"textprep"},{"location":"api/textprep.html#tokenwiser.textprep._cleaner.Cleaner","text":"Applies a lowercase and removes non-alphanum. Usage: from tokenwiser.textprep import Cleaner single = Cleaner () . encode_single ( \"$$$5 dollars\" ) assert single == \"5 dollars\" multi = Cleaner () . transform ([ \"$$$5 dollars\" , \"#hashtag!\" ]) assert multi == [ \"5 dollars\" , \"hashtag\" ]","title":"Cleaner"},{"location":"api/textprep.html#tokenwiser.textprep._identity.Identity","text":"Keeps the text as is. Can be used as a placeholder in a pipeline. Usage: from tokenwiser.textprep import Identity text = [ \"hello\" , \"world\" ] example = Identity () . transform ( text ) assert example == [ \"hello\" , \"world\" ] The main use-case is as a placeholder. from tokenwiser.pipeline import make_concat from sklearn.pipeline import make_pipeline , make_union from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), )","title":"Identity"},{"location":"api/textprep.html#tokenwiser.textprep._identity.Identity.fit","text":"Fits the TextPrep step. Considered a no-op. Source code in tokenwiser/textprep/_identity.py def fit ( self , X , y = None ): return self","title":"fit()"},{"location":"api/textprep.html#tokenwiser.textprep._identity.Identity.partial_fit","text":"Partially fits the TextPrep step. Considered a no-op. Source code in tokenwiser/textprep/_identity.py def partial_fit ( self , X , y = None ): return self","title":"partial_fit()"},{"location":"api/textprep.html#tokenwiser.textprep._hyphen.HyphenTextPrep","text":"Hyphenate the text going in. Usage: from tokenwiser.textprep import HyphenTextPrep multi = HyphenTextPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ]","title":"HyphenTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._phonetic.PhoneticTextPrep","text":"The ProneticPrep object prepares strings by encoding them phonetically. Parameters: Name Type Description Default kind type of encoding, either \"soundex\" , \" metaphone \" or \"nysiis\" required Usage: import spacy from tokenwiser.textprep import PhoneticTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = PhoneticTextPrep ( kind = \"soundex\" ) . transform ([ \"dinosaurus book\" ]) example2 = PhoneticTextPrep ( kind = \"metaphone\" ) . transform ([ \"dinosaurus book\" ]) example3 = PhoneticTextPrep ( kind = \"nysiis\" ) . transform ([ \"dinosaurus book\" ]) assert example1 [ 0 ] == 'D526 B200' assert example2 [ 0 ] == 'TNSRS BK' assert example3 [ 0 ] == 'DANASAR BAC'","title":"PhoneticTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._yake.YakeTextPrep","text":"Remove all text except meaningful key-phrases. Uses yake . Parameters: Name Type Description Default top_n number of key-phrases to select required unique only return unique keywords from the key-phrases required Usage: from tokenwiser.textprep import YakeTextPrep text = [ \"Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning\" ] example = YakeTextPrep ( top_n = 3 , unique = False ) . transform ( text ) assert example [ 0 ] == 'hosts data science acquiring kaggle google is acquiring'","title":"YakeTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._morph.SpacyMorphTextPrep","text":"Adds morphologic information to tokens in text. Usage: import spacy from tokenwiser.textprep import SpacyMorphTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyMorphTextPrep ( nlp ) . encode_single ( \"quick! duck!\" ) example2 = SpacyMorphTextPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"quick|Degree=Pos !|PunctType=Peri duck|Number=Sing !|PunctType=Peri\" assert example2 == \"hey| look|VerbForm=Inf a|Definite=Ind|PronType=Art duck|Number=Sing\"","title":"SpacyMorphTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._morph.SpacyPosTextPrep","text":"Adds part of speech information per token using spaCy. Parameters: Name Type Description Default model the spaCy model to use required lemma also lemmatize the text required fine_grained use fine grained parts of speech required Usage: import spacy from tokenwiser.textprep import SpacyPosTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyPosTextPrep ( nlp ) . encode_single ( \"we need to duck\" ) example2 = SpacyPosTextPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"we|PRON need|VERB to|PART duck|VERB\" assert example2 == \"hey|INTJ look|VERB a|DET duck|NOUN\"","title":"SpacyPosTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._morph.SpacyLemmaTextPrep","text":"Turns each token into a lemmatizer version using spaCy. Usage: import spacy from tokenwiser.textprep import SpacyLemmaTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyLemmaTextPrep ( nlp ) . encode_single ( \"we are running\" ) example2 = SpacyLemmaTextPrep ( nlp ) . encode_single ( \"these are dogs\" ) assert example1 == 'we be run' assert example2 == 'these be dog'","title":"SpacyLemmaTextPrep"},{"location":"guide/sklearn.html","text":"Scikit-Learn pipelines are amazing but they are not perfect for simple text use-cases. The standard pipeline does not allow for interactive learning. You can apply .fit but that's it. Even if the tools inside of the pipeline have a .partial_fit available, the pipeline doesn't allow it. The CountVectorizer is great, but we might need some more text-tricks at our disposal that are specialized towards text to make this object more effective. Part of what this library does is give more tools that extend scikit-learn for simple text classification problems. In this document we will showcase some of the main features. Text Preparation Tools \u00b6 Let's first discuss a basic pipeline for text inside of scikit-learn. Base Pipeline \u00b6 This simplest text classification pipeline in scikit-learn looks like this; from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import SGDClassifier pipe = make_pipeline ( CountVectorizer (), SGDClassifier () ) This pipeline will encode words as sparse features before passing them on to the logistic regression model. This pattern is very common and has proven to work well enough for many English text classification tasks. The nice thing about using a SGDClassifier is that we're able to learn from our data even if the dataset does not fit in memory. We can call .partial_fit instead of .fit and learn in a more \"online\" setting. That said, there are things we can do even to this pipeline to make it better. Spelling Errors \u00b6 When you are classifying online texts you are often confronted with spelling errors. To deal with this you'd typically use a CountVectorizer with a character-level analyzer such that you also encode subwords. With all of these subwords around, we'll be more robust against spelling errors. The downside of this approach is that you might wonder if we really need all these subwords. So how about this, let's add a step that will turn our text into subwords by splitting up hyphens. from tokenwiser.textprep import HyphenTextPrep multi = HyphenTextPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ] The HyphenTextPrep preprocessor is a TextPrep -object. For all intents and purposes these are scikit-learn compatible preprocessing components but they all output strings instead of arrays. What's nice about these though is that you can \"retokenize\" the original text. This allows you to use the subtokens as if they were tokens which might help keep your pipelines lightweight while still keeping them robust against certain spelling errors. Long Texts \u00b6 There are some other tricks that you might want to apply for longer texts. Maybe you want to summarise a text before vectorizing it. So maybe it'd be nice to use a transformer that keeps only the most important tokens. A neat heuristic toolkit for this is yake (you can find a demo here ). This package also features a scikit-learn compatible component for it. from tokenwiser.textprep import YakeTextPrep text = [ \"Sources tell us that Google is acquiring Kaggle, \\ a platform that hosts data science and machine learning\" ] example = YakeTextPrep ( top_n = 3 , unique = False ) . transform ( text ) assert example [ 0 ] == 'hosts data science acquiring kaggle google is acquiring' The idea here is to reduce the text down to only the most important words. Again, this trick might keep the algorithm lightweight and this trick will go a lot further than most \"stopword\"-lists. Bag of Tricks! \u00b6 The goal of this library is to host a few meaningful tricks that might be helpful. Here's some more; Cleaner lowercase text remove all non alphanumeric characters. Identity just keeps the text as is, useful when constructing elaborate pipelines. PhoneticTextPrep translate text into a phonetic encoding. SpacyPosTextPrep add part of speech infomation to the text using spaCy. SpacyLemmaTextPrep lemmatize the text using spaCy. All of these tools are part of the textprep submodule and are documented in detail here . Pipeline Tools \u00b6 Pipeline components are certainly nice. But maybe we can go a step further for text. Maybe we can make better pipelines for text too! Concatenate Text \u00b6 In scikit-learn you would use FeatureUnion or make_union to concatenate features in a pipeline. Ut is assumed that transformers output arrays that need to be concatenated so the result of a concatenation is always a 2D array. This can be a bit awkward if you're using text preprocessors. The reason why we want to keep everything a string is so that the CountVectorizer from scikit-learn can properly encode it. That is why this library comes with a special union component: TextConcat . It concatenates the output of text-prep tools into a string instead of an array. Note that we also pack a convenient make_concat function too. from sklearn.pipeline import make_pipeline from tokenwiser.pipeline import make_concat from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), ) output = pipe . fit_transform ([ \"hello astrology!!!!\" ]) assert output == [ 'hello astrology hel lo astro logy' ] Again, we see that we're taking a text input and that we're generating a text output. The make_concat is making sure that we concatenate strings, not arrays! This is great when we want to follow up with a `CountVectorizer! from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.pipeline import make_concat from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), CountVectorizer (), LogisticRegression () ) The mental picture for pipe -pipeline looks like the diagram below. Partial Fit \u00b6 We can go a step further though. The scikit-learn pipeline follows the fit/predict API. That means that we cannot use .partial_fit () . Even if all the components in the pipeline are compatible with the partial_fit/predict API. That is why this library also introduced components for mini-batch learning: PartialPipeline and make_partial_pipeline In these scenarios you will need to swap out the CountVectorizer with a HashVectorizer in order to be able to learn from new data comming in. from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.pipeline import make_concat , make_partial_pipeline from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_partial_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), HashingVectorizer (), LogisticRegression () ) This pipe -Pipeline is scikit-learn compatible for all intents and purposes but it has the option of learning from batches of data via partal_fit . This is great because it means that you're able to classify text even when it doesn't fit into memory! Note that all of the TextPrep -components in this library allow for partial_fit .","title":"Scikit-Learn"},{"location":"guide/sklearn.html#text-preparation-tools","text":"Let's first discuss a basic pipeline for text inside of scikit-learn.","title":"Text Preparation Tools"},{"location":"guide/sklearn.html#base-pipeline","text":"This simplest text classification pipeline in scikit-learn looks like this; from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import SGDClassifier pipe = make_pipeline ( CountVectorizer (), SGDClassifier () ) This pipeline will encode words as sparse features before passing them on to the logistic regression model. This pattern is very common and has proven to work well enough for many English text classification tasks. The nice thing about using a SGDClassifier is that we're able to learn from our data even if the dataset does not fit in memory. We can call .partial_fit instead of .fit and learn in a more \"online\" setting. That said, there are things we can do even to this pipeline to make it better.","title":"Base Pipeline"},{"location":"guide/sklearn.html#spelling-errors","text":"When you are classifying online texts you are often confronted with spelling errors. To deal with this you'd typically use a CountVectorizer with a character-level analyzer such that you also encode subwords. With all of these subwords around, we'll be more robust against spelling errors. The downside of this approach is that you might wonder if we really need all these subwords. So how about this, let's add a step that will turn our text into subwords by splitting up hyphens. from tokenwiser.textprep import HyphenTextPrep multi = HyphenTextPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ] The HyphenTextPrep preprocessor is a TextPrep -object. For all intents and purposes these are scikit-learn compatible preprocessing components but they all output strings instead of arrays. What's nice about these though is that you can \"retokenize\" the original text. This allows you to use the subtokens as if they were tokens which might help keep your pipelines lightweight while still keeping them robust against certain spelling errors.","title":"Spelling Errors"},{"location":"guide/sklearn.html#long-texts","text":"There are some other tricks that you might want to apply for longer texts. Maybe you want to summarise a text before vectorizing it. So maybe it'd be nice to use a transformer that keeps only the most important tokens. A neat heuristic toolkit for this is yake (you can find a demo here ). This package also features a scikit-learn compatible component for it. from tokenwiser.textprep import YakeTextPrep text = [ \"Sources tell us that Google is acquiring Kaggle, \\ a platform that hosts data science and machine learning\" ] example = YakeTextPrep ( top_n = 3 , unique = False ) . transform ( text ) assert example [ 0 ] == 'hosts data science acquiring kaggle google is acquiring' The idea here is to reduce the text down to only the most important words. Again, this trick might keep the algorithm lightweight and this trick will go a lot further than most \"stopword\"-lists.","title":"Long Texts"},{"location":"guide/sklearn.html#bag-of-tricks","text":"The goal of this library is to host a few meaningful tricks that might be helpful. Here's some more; Cleaner lowercase text remove all non alphanumeric characters. Identity just keeps the text as is, useful when constructing elaborate pipelines. PhoneticTextPrep translate text into a phonetic encoding. SpacyPosTextPrep add part of speech infomation to the text using spaCy. SpacyLemmaTextPrep lemmatize the text using spaCy. All of these tools are part of the textprep submodule and are documented in detail here .","title":"Bag of Tricks!"},{"location":"guide/sklearn.html#pipeline-tools","text":"Pipeline components are certainly nice. But maybe we can go a step further for text. Maybe we can make better pipelines for text too!","title":"Pipeline Tools"},{"location":"guide/sklearn.html#concatenate-text","text":"In scikit-learn you would use FeatureUnion or make_union to concatenate features in a pipeline. Ut is assumed that transformers output arrays that need to be concatenated so the result of a concatenation is always a 2D array. This can be a bit awkward if you're using text preprocessors. The reason why we want to keep everything a string is so that the CountVectorizer from scikit-learn can properly encode it. That is why this library comes with a special union component: TextConcat . It concatenates the output of text-prep tools into a string instead of an array. Note that we also pack a convenient make_concat function too. from sklearn.pipeline import make_pipeline from tokenwiser.pipeline import make_concat from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), ) output = pipe . fit_transform ([ \"hello astrology!!!!\" ]) assert output == [ 'hello astrology hel lo astro logy' ] Again, we see that we're taking a text input and that we're generating a text output. The make_concat is making sure that we concatenate strings, not arrays! This is great when we want to follow up with a `CountVectorizer! from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.pipeline import make_concat from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), CountVectorizer (), LogisticRegression () ) The mental picture for pipe -pipeline looks like the diagram below.","title":"Concatenate Text"},{"location":"guide/sklearn.html#partial-fit","text":"We can go a step further though. The scikit-learn pipeline follows the fit/predict API. That means that we cannot use .partial_fit () . Even if all the components in the pipeline are compatible with the partial_fit/predict API. That is why this library also introduced components for mini-batch learning: PartialPipeline and make_partial_pipeline In these scenarios you will need to swap out the CountVectorizer with a HashVectorizer in order to be able to learn from new data comming in. from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.pipeline import make_concat , make_partial_pipeline from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_partial_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), HashingVectorizer (), LogisticRegression () ) This pipe -Pipeline is scikit-learn compatible for all intents and purposes but it has the option of learning from batches of data via partal_fit . This is great because it means that you're able to classify text even when it doesn't fit into memory! Note that all of the TextPrep -components in this library allow for partial_fit .","title":"Partial Fit"},{"location":"guide/spacy.html","text":"This is where we'll elaborate on the spaCy tools. Under construction.","title":"spaCy"}]}