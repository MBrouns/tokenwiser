{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"tokenwiser \u00b6 Bag of, not words, but tricks! This project contains a couple of scikit-learn compatible \"tricks\" that I've used in NLP experiments. It's a collection of tricks for sparse data. SubModules \u00b6 .prep : Contains string pre-processing tools. Takes a string in and pushes a string out. .tok : Contains things that take stings and output iterables. We also have some extra utilities. Featurizer \u00b6 Make a simple featurizer that will check if a word appears. Make a simple featurizer that will check if a regex appears.","title":"Home"},{"location":"index.html#tokenwiser","text":"Bag of, not words, but tricks! This project contains a couple of scikit-learn compatible \"tricks\" that I've used in NLP experiments. It's a collection of tricks for sparse data.","title":"tokenwiser"},{"location":"index.html#submodules","text":".prep : Contains string pre-processing tools. Takes a string in and pushes a string out. .tok : Contains things that take stings and output iterables. We also have some extra utilities.","title":"SubModules"},{"location":"index.html#featurizer","text":"Make a simple featurizer that will check if a word appears. Make a simple featurizer that will check if a regex appears.","title":"Featurizer"},{"location":"prep.html","text":"tokenisers \u00b6 Tokenisers are pipeline components that take a string and output a list of strings. Cleaner \u00b6 Applies a lowercase and removes non-alphanum. Usage: from tokenwiser.prep import Cleaner single = Cleaner () . encode_single ( \"$$$5 dollars\" ) assert single == \"5 dollars\" multi = Cleaner () . transform ([ \"$$$5 dollars\" , \"#hashtag!\" ]) assert multi == [ \"5 dollars\" , \"hashtag\" ] HyphenPrep \u00b6 Hyphenate the text going in. Usage: from tokenwiser.prep import HyphenPrep single = HyphenPrep () . encode_single ( \"dinosaurhead\" ) assert single == \"di no saur head\" multi = HyphenPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ] PhoneticPrep \u00b6 The ProneticPrep object prepares strings by encoding them phonetically. Parameters: Name Type Description Default kind type of encoding, either \"soundex\" , \" metaphone \" or \"nysiis\" required Usage: from tokenwiser.prep import PhoneticPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = PhoneticPrep ( kind = \"soundex\" ) . encode_single ( \"dinosaurus book\" ) example2 = PhoneticPrep ( kind = \"metaphone\" ) . encode_single ( \"dinosaurus book\" ) example3 = PhoneticPrep ( kind = \"nysiis\" ) . encode_single ( \"dinosaurus book\" ) assert example1 == 'D526 B200' assert example2 == 'TNSRS BK' assert example3 == 'DANASAR BAC' SpacyMorphPrep \u00b6 Adds morphologic information to tokens in text. Usage: import spacy from tokenwiser.prep import SpacyMorphPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyMorphPrep ( nlp ) . encode_single ( \"quick! duck!\" ) example2 = SpacyMorphPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"quick|Degree=Pos !|PunctType=Peri duck|Number=Sing !|PunctType=Peri\" assert example2 == \"hey| look|VerbForm=Inf a|Definite=Ind|PronType=Art duck|Number=Sing\" SpacyPosPrep \u00b6 Adds part of speech information per token using spaCy. Parameters: Name Type Description Default model the spaCy model to use required lemma also lemmatize the text required Usage: import spacy from tokenwiser.prep import SpacyPosPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyPosPrep ( nlp ) . encode_single ( \"we need to duck\" ) example2 = SpacyPosPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"we|PRON need|VERB to|PART duck|VERB\" assert example2 == \"hey|INTJ look|VERB a|DET duck|NOUN\" SpacyLemmaPrep \u00b6 Turns each token into a lemmatizer version using spaCy. Usage: import spacy from tokenwiser.prep import SpacyLemmaPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyLemmaPrep ( nlp ) . encode_single ( \"we are running\" ) example2 = SpacyLemmaPrep ( nlp ) . encode_single ( \"these are dogs\" ) assert example1 == 'we be run' assert example2 == 'these be dog'","title":"prep"},{"location":"prep.html#tokenisers","text":"Tokenisers are pipeline components that take a string and output a list of strings.","title":"tokenisers"},{"location":"prep.html#tokenwiser.prep._cleaner.Cleaner","text":"Applies a lowercase and removes non-alphanum. Usage: from tokenwiser.prep import Cleaner single = Cleaner () . encode_single ( \"$$$5 dollars\" ) assert single == \"5 dollars\" multi = Cleaner () . transform ([ \"$$$5 dollars\" , \"#hashtag!\" ]) assert multi == [ \"5 dollars\" , \"hashtag\" ]","title":"Cleaner"},{"location":"prep.html#tokenwiser.prep._hyphen.HyphenPrep","text":"Hyphenate the text going in. Usage: from tokenwiser.prep import HyphenPrep single = HyphenPrep () . encode_single ( \"dinosaurhead\" ) assert single == \"di no saur head\" multi = HyphenPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ]","title":"HyphenPrep"},{"location":"prep.html#tokenwiser.prep._phonetic.PhoneticPrep","text":"The ProneticPrep object prepares strings by encoding them phonetically. Parameters: Name Type Description Default kind type of encoding, either \"soundex\" , \" metaphone \" or \"nysiis\" required Usage: from tokenwiser.prep import PhoneticPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = PhoneticPrep ( kind = \"soundex\" ) . encode_single ( \"dinosaurus book\" ) example2 = PhoneticPrep ( kind = \"metaphone\" ) . encode_single ( \"dinosaurus book\" ) example3 = PhoneticPrep ( kind = \"nysiis\" ) . encode_single ( \"dinosaurus book\" ) assert example1 == 'D526 B200' assert example2 == 'TNSRS BK' assert example3 == 'DANASAR BAC'","title":"PhoneticPrep"},{"location":"prep.html#tokenwiser.prep._morph.SpacyMorphPrep","text":"Adds morphologic information to tokens in text. Usage: import spacy from tokenwiser.prep import SpacyMorphPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyMorphPrep ( nlp ) . encode_single ( \"quick! duck!\" ) example2 = SpacyMorphPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"quick|Degree=Pos !|PunctType=Peri duck|Number=Sing !|PunctType=Peri\" assert example2 == \"hey| look|VerbForm=Inf a|Definite=Ind|PronType=Art duck|Number=Sing\"","title":"SpacyMorphPrep"},{"location":"prep.html#tokenwiser.prep._morph.SpacyPosPrep","text":"Adds part of speech information per token using spaCy. Parameters: Name Type Description Default model the spaCy model to use required lemma also lemmatize the text required Usage: import spacy from tokenwiser.prep import SpacyPosPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyPosPrep ( nlp ) . encode_single ( \"we need to duck\" ) example2 = SpacyPosPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"we|PRON need|VERB to|PART duck|VERB\" assert example2 == \"hey|INTJ look|VERB a|DET duck|NOUN\"","title":"SpacyPosPrep"},{"location":"prep.html#tokenwiser.prep._morph.SpacyLemmaPrep","text":"Turns each token into a lemmatizer version using spaCy. Usage: import spacy from tokenwiser.prep import SpacyLemmaPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyLemmaPrep ( nlp ) . encode_single ( \"we are running\" ) example2 = SpacyLemmaPrep ( nlp ) . encode_single ( \"these are dogs\" ) assert example1 == 'we be run' assert example2 == 'these be dog'","title":"SpacyLemmaPrep"},{"location":"snippets.html","text":"Here's a few useful snippets to keep around. Conversion \u00b6 Rasa NLU to DataFrame \u00b6 import json import pathlib import pandas as pd def nlu_folder_to_dataframe ( path = \"/Users/vincent/Development/rasa-demo/data/nlu/\" ): from rasa.nlu.convert import convert_training_data data = [] for p in pathlib . Path ( path ) . glob ( \"*.md\" ): name = p . parts [ - 1 ] name = name [: name . find ( \".\" )] convert_training_data ( str ( p ), f \" { name } .json\" , output_format = \"json\" , language = \"en\" ) blob = json . loads ( pathlib . Path ( f \" { name } .json\" ) . read_text ()) for d in blob [ 'rasa_nlu_data' ][ 'common_examples' ]: data . append ({ 'text' : d [ 'text' ], 'label' : d [ 'intent' ]}) pathlib . Path ( f \" { name } .json\" ) . unlink () return pd . DataFrame ( data ) nlu_folder_to_dataframe ()","title":"Snippets"},{"location":"snippets.html#conversion","text":"","title":"Conversion"},{"location":"snippets.html#rasa-nlu-to-dataframe","text":"import json import pathlib import pandas as pd def nlu_folder_to_dataframe ( path = \"/Users/vincent/Development/rasa-demo/data/nlu/\" ): from rasa.nlu.convert import convert_training_data data = [] for p in pathlib . Path ( path ) . glob ( \"*.md\" ): name = p . parts [ - 1 ] name = name [: name . find ( \".\" )] convert_training_data ( str ( p ), f \" { name } .json\" , output_format = \"json\" , language = \"en\" ) blob = json . loads ( pathlib . Path ( f \" { name } .json\" ) . read_text ()) for d in blob [ 'rasa_nlu_data' ][ 'common_examples' ]: data . append ({ 'text' : d [ 'text' ], 'label' : d [ 'intent' ]}) pathlib . Path ( f \" { name } .json\" ) . unlink () return pd . DataFrame ( data ) nlu_folder_to_dataframe ()","title":"Rasa NLU to DataFrame"},{"location":"tok.html","text":"tokenisers \u00b6 Tokenisers are pipeline components that take a string and output a list of strings. WhiteSpaceTokenizer \u00b6 A simple tokenizer that simple splits on whitespace. Usage: from tokenwiser.tok import WhiteSpaceTokenizer tok = WhiteSpaceTokenizer () single = tok ( \"hello world\" ) assert single == [ \"hello\" , \"world\" ] SpacyTokenizer \u00b6 A tokenizer that uses spaCy under the hood for the tokenization. Parameters: Name Type Description Default model reference to the spaCy model required lemma weather or not to also apply lemmatization required stop weather or not to remove stopwords required Usage: import spacy from tokenwiser.tok import SpacyTokenizer # This can also be a Non-English model. nlp = spacy . load ( \"en_core_web_sm\" ) tok = SpacyTokenizer ( model = nlp ) single = tok ( \"hello world\" ) assert single == [ \"hello\" , \"world\" ]","title":"tok"},{"location":"tok.html#tokenisers","text":"Tokenisers are pipeline components that take a string and output a list of strings.","title":"tokenisers"},{"location":"tok.html#tokenwiser.tok._whitespace.WhiteSpaceTokenizer","text":"A simple tokenizer that simple splits on whitespace. Usage: from tokenwiser.tok import WhiteSpaceTokenizer tok = WhiteSpaceTokenizer () single = tok ( \"hello world\" ) assert single == [ \"hello\" , \"world\" ]","title":"WhiteSpaceTokenizer"},{"location":"tok.html#tokenwiser.tok._spacy.SpacyTokenizer","text":"A tokenizer that uses spaCy under the hood for the tokenization. Parameters: Name Type Description Default model reference to the spaCy model required lemma weather or not to also apply lemmatization required stop weather or not to remove stopwords required Usage: import spacy from tokenwiser.tok import SpacyTokenizer # This can also be a Non-English model. nlp = spacy . load ( \"en_core_web_sm\" ) tok = SpacyTokenizer ( model = nlp ) single = tok ( \"hello world\" ) assert single == [ \"hello\" , \"world\" ]","title":"SpacyTokenizer"}]}