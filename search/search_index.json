{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"tokenwiser \u00b6 Bag of, not words, but tricks! This project contains a couple of scikit-learn compatible \"tricks\" that I've used in NLP experiments. It's a collection of tricks for sparse data. SubModules \u00b6 .prep : Contains string pre-processing tools. Takes a string in and pushes a string out. .tok : Contains things that take stings and output iterables. We also have some extra utilities. Featurizer \u00b6 Make a simple featurizer that will check if a word appears. Make a simple featurizer that will check if a regex appears.","title":"Home"},{"location":"index.html#tokenwiser","text":"Bag of, not words, but tricks! This project contains a couple of scikit-learn compatible \"tricks\" that I've used in NLP experiments. It's a collection of tricks for sparse data.","title":"tokenwiser"},{"location":"index.html#submodules","text":".prep : Contains string pre-processing tools. Takes a string in and pushes a string out. .tok : Contains things that take stings and output iterables. We also have some extra utilities.","title":"SubModules"},{"location":"index.html#featurizer","text":"Make a simple featurizer that will check if a word appears. Make a simple featurizer that will check if a regex appears.","title":"Featurizer"},{"location":"prep.html","text":"tokenisers \u00b6 Tokenisers are pipeline components that take a string and output a list of strings. Cleaner \u00b6 Applies a lowercase and removes non-alphanum. Usage: from tokenwiser.prep import Cleaner single = Cleaner () . encode_single ( \"$$$5 dollars\" ) assert single == \"5 dollars\" multi = Cleaner () . transform ([ \"$$$5 dollars\" , \"#hashtag!\" ]) assert multi == [ \"5 dollars\" , \"hashtag\" ] HyphenPrep \u00b6 Hyphenate the text going in. Usage: from tokenwiser.prep import HyphenPrep single = HyphenPrep () . encode_single ( \"dinosaurhead\" ) assert single == \"di no saur head\" multi = HyphenPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ] PhoneticPrep \u00b6 The ProneticPrep object prepares strings by encoding them phonetically. Parameters: Name Type Description Default kind type of encoding, either \"soundex\" , \" metaphone \" or \"nysiis\" required Usage: from tokenwiser.prep import PhoneticPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = PhoneticPrep ( kind = \"soundex\" ) . encode_single ( \"dinosaurus book\" ) example2 = PhoneticPrep ( kind = \"metaphone\" ) . encode_single ( \"dinosaurus book\" ) example3 = PhoneticPrep ( kind = \"nysiis\" ) . encode_single ( \"dinosaurus book\" ) assert example1 == 'D526 B200' assert example2 == 'TNSRS BK' assert example3 == 'DANASAR BAC' SpacyMorphPrep \u00b6 Adds morphologic information to tokens in text. Usage: import spacy from tokenwiser.prep import SpacyMorphPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyMorphPrep ( nlp ) . encode_single ( \"quick! duck!\" ) example2 = SpacyMorphPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"quick|Degree=Pos !|PunctType=Peri duck|Number=Sing !|PunctType=Peri\" assert example2 == \"hey| look|VerbForm=Inf a|Definite=Ind|PronType=Art duck|Number=Sing\" SpacyPosPrep \u00b6 Adds part of speech information per token using spaCy. Parameters: Name Type Description Default model the spaCy model to use required lemma also lemmatize the text required Usage: import spacy from tokenwiser.prep import SpacyPosPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyPosPrep ( nlp ) . encode_single ( \"we need to duck\" ) example2 = SpacyPosPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"we|PRON need|VERB to|PART duck|VERB\" assert example2 == \"hey|INTJ look|VERB a|DET duck|NOUN\" SpacyLemmaPrep \u00b6 Turns each token into a lemmatizer version using spaCy. Usage: import spacy from tokenwiser.prep import SpacyLemmaPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyLemmaPrep ( nlp ) . encode_single ( \"we are running\" ) example2 = SpacyLemmaPrep ( nlp ) . encode_single ( \"these are dogs\" ) assert example1 == 'we be run' assert example2 == 'these be dog'","title":"prep"},{"location":"prep.html#tokenisers","text":"Tokenisers are pipeline components that take a string and output a list of strings.","title":"tokenisers"},{"location":"prep.html#tokenwiser.prep._cleaner.Cleaner","text":"Applies a lowercase and removes non-alphanum. Usage: from tokenwiser.prep import Cleaner single = Cleaner () . encode_single ( \"$$$5 dollars\" ) assert single == \"5 dollars\" multi = Cleaner () . transform ([ \"$$$5 dollars\" , \"#hashtag!\" ]) assert multi == [ \"5 dollars\" , \"hashtag\" ]","title":"Cleaner"},{"location":"prep.html#tokenwiser.prep._hyphen.HyphenPrep","text":"Hyphenate the text going in. Usage: from tokenwiser.prep import HyphenPrep single = HyphenPrep () . encode_single ( \"dinosaurhead\" ) assert single == \"di no saur head\" multi = HyphenPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ]","title":"HyphenPrep"},{"location":"prep.html#tokenwiser.prep._phonetic.PhoneticPrep","text":"The ProneticPrep object prepares strings by encoding them phonetically. Parameters: Name Type Description Default kind type of encoding, either \"soundex\" , \" metaphone \" or \"nysiis\" required Usage: from tokenwiser.prep import PhoneticPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = PhoneticPrep ( kind = \"soundex\" ) . encode_single ( \"dinosaurus book\" ) example2 = PhoneticPrep ( kind = \"metaphone\" ) . encode_single ( \"dinosaurus book\" ) example3 = PhoneticPrep ( kind = \"nysiis\" ) . encode_single ( \"dinosaurus book\" ) assert example1 == 'D526 B200' assert example2 == 'TNSRS BK' assert example3 == 'DANASAR BAC'","title":"PhoneticPrep"},{"location":"prep.html#tokenwiser.prep._morph.SpacyMorphPrep","text":"Adds morphologic information to tokens in text. Usage: import spacy from tokenwiser.prep import SpacyMorphPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyMorphPrep ( nlp ) . encode_single ( \"quick! duck!\" ) example2 = SpacyMorphPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"quick|Degree=Pos !|PunctType=Peri duck|Number=Sing !|PunctType=Peri\" assert example2 == \"hey| look|VerbForm=Inf a|Definite=Ind|PronType=Art duck|Number=Sing\"","title":"SpacyMorphPrep"},{"location":"prep.html#tokenwiser.prep._morph.SpacyPosPrep","text":"Adds part of speech information per token using spaCy. Parameters: Name Type Description Default model the spaCy model to use required lemma also lemmatize the text required Usage: import spacy from tokenwiser.prep import SpacyPosPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyPosPrep ( nlp ) . encode_single ( \"we need to duck\" ) example2 = SpacyPosPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"we|PRON need|VERB to|PART duck|VERB\" assert example2 == \"hey|INTJ look|VERB a|DET duck|NOUN\"","title":"SpacyPosPrep"},{"location":"prep.html#tokenwiser.prep._morph.SpacyLemmaPrep","text":"Turns each token into a lemmatizer version using spaCy. Usage: import spacy from tokenwiser.prep import SpacyLemmaPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyLemmaPrep ( nlp ) . encode_single ( \"we are running\" ) example2 = SpacyLemmaPrep ( nlp ) . encode_single ( \"these are dogs\" ) assert example1 == 'we be run' assert example2 == 'these be dog'","title":"SpacyLemmaPrep"},{"location":"snippets.html","text":"Here's a few useful snippets to keep around. Scikit-Learn in SpaCy \u00b6 You can add a custom component if you're really keen. import spacy import pandas as pd from spacy.language import Language from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline , make_union df = pd . read_json ( \"data/intents.jsonl\" , lines = True ) feat = make_union ( CountVectorizer ()) pipe = make_pipeline ( feat , LogisticRegression ()) pipe . fit ( df [ 'text' ], df [ 'label' ]) nlp = spacy . load ( \"en_core_web_md\" ) @Language . component ( \"sklearn-cat\" ) def my_component ( doc ): pred = pipe . predict ([ doc . text ])[ 0 ] proba = pipe . predict_proba ([ doc . text ]) . max () doc . cats [ pred ] = proba return doc nlp . add_pipe ( \"sklearn-cat\" ) nlp ( \"can you flip a coin?\" ) . cats # {'flip_coin': 0.9747356912446946}","title":"Snippets"},{"location":"snippets.html#scikit-learn-in-spacy","text":"You can add a custom component if you're really keen. import spacy import pandas as pd from spacy.language import Language from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline , make_union df = pd . read_json ( \"data/intents.jsonl\" , lines = True ) feat = make_union ( CountVectorizer ()) pipe = make_pipeline ( feat , LogisticRegression ()) pipe . fit ( df [ 'text' ], df [ 'label' ]) nlp = spacy . load ( \"en_core_web_md\" ) @Language . component ( \"sklearn-cat\" ) def my_component ( doc ): pred = pipe . predict ([ doc . text ])[ 0 ] proba = pipe . predict_proba ([ doc . text ]) . max () doc . cats [ pred ] = proba return doc nlp . add_pipe ( \"sklearn-cat\" ) nlp ( \"can you flip a coin?\" ) . cats # {'flip_coin': 0.9747356912446946}","title":"Scikit-Learn in SpaCy"},{"location":"tok.html","text":"tokenisers \u00b6 Tokenisers are pipeline components that take a string and output a list of strings. WhiteSpaceTokenizer \u00b6 A simple tokenizer that simple splits on whitespace. Usage: from tokenwiser.tok import WhiteSpaceTokenizer tok = WhiteSpaceTokenizer () single = tok ( \"hello world\" ) assert single == [ \"hello\" , \"world\" ] SpacyTokenizer \u00b6 A tokenizer that uses spaCy under the hood for the tokenization. Parameters: Name Type Description Default model reference to the spaCy model required lemma weather or not to also apply lemmatization required stop weather or not to remove stopwords required Usage: import spacy from tokenwiser.tok import SpacyTokenizer # This can also be a Non-English model. nlp = spacy . load ( \"en_core_web_sm\" ) tok = SpacyTokenizer ( model = nlp ) single = tok ( \"hello world\" ) assert single == [ \"hello\" , \"world\" ]","title":"tok"},{"location":"tok.html#tokenisers","text":"Tokenisers are pipeline components that take a string and output a list of strings.","title":"tokenisers"},{"location":"tok.html#tokenwiser.tok._whitespace.WhiteSpaceTokenizer","text":"A simple tokenizer that simple splits on whitespace. Usage: from tokenwiser.tok import WhiteSpaceTokenizer tok = WhiteSpaceTokenizer () single = tok ( \"hello world\" ) assert single == [ \"hello\" , \"world\" ]","title":"WhiteSpaceTokenizer"},{"location":"tok.html#tokenwiser.tok._spacy.SpacyTokenizer","text":"A tokenizer that uses spaCy under the hood for the tokenization. Parameters: Name Type Description Default model reference to the spaCy model required lemma weather or not to also apply lemmatization required stop weather or not to remove stopwords required Usage: import spacy from tokenwiser.tok import SpacyTokenizer # This can also be a Non-English model. nlp = spacy . load ( \"en_core_web_sm\" ) tok = SpacyTokenizer ( model = nlp ) single = tok ( \"hello world\" ) assert single == [ \"hello\" , \"world\" ]","title":"SpacyTokenizer"}]}