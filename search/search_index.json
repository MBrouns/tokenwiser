{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"tokenwiser \u00b6 Bag of, not words, but tricks! This project contains a couple of scikit-learn compatible \"tricks\" that I've used in NLP experiments. It's a collection of tricks for sparse data. SubModules \u00b6 .prep : Contains string pre-processing tools. Takes a string in and pushes a string out. .tok : Contains things that take stings and output iterables. We also have some extra utilities. Featurizer \u00b6 Make a simple featurizer that will check if a word appears. Make a simple featurizer that will check if a regex appears.","title":"Home"},{"location":"index.html#tokenwiser","text":"Bag of, not words, but tricks! This project contains a couple of scikit-learn compatible \"tricks\" that I've used in NLP experiments. It's a collection of tricks for sparse data.","title":"tokenwiser"},{"location":"index.html#submodules","text":".prep : Contains string pre-processing tools. Takes a string in and pushes a string out. .tok : Contains things that take stings and output iterables. We also have some extra utilities.","title":"SubModules"},{"location":"index.html#featurizer","text":"Make a simple featurizer that will check if a word appears. Make a simple featurizer that will check if a regex appears.","title":"Featurizer"},{"location":"goal.html","text":"We noticed that a lot of benchmarks relied on heavy-weight tools while they did not check if something more lightweight would also work. In this package we list tools that will help you do lightweight benchmarks using three packages: scikit-learn spaCy vowpal wabbit The idea is that maybe we don't need language models for basic tasks. Maybe we just need to add some features from spaCy in scikit-learn. Conversely, maybe we can add some simple benchmark models to spaCy by leveraging scikit-learn and vowpal wabbit. This is what we hope to explore in this package. ps. If you're looking for a tool that can add language models to scikit-learn pipelines as a benchmark you'll want to explore another tool: whatlies .","title":"Goal"},{"location":"prep.html","text":"tokenisers \u00b6 Tokenisers are pipeline components that take a string and output a list of strings. Cleaner \u00b6 Applies a lowercase and removes non-alphanum. Usage: from tokenwiser.prep import Cleaner single = Cleaner () . encode_single ( \"$$$5 dollars\" ) assert single == \"5 dollars\" multi = Cleaner () . transform ([ \"$$$5 dollars\" , \"#hashtag!\" ]) assert multi == [ \"5 dollars\" , \"hashtag\" ] HyphenPrep \u00b6 Hyphenate the text going in. Usage: from tokenwiser.prep import HyphenPrep single = HyphenPrep () . encode_single ( \"dinosaurhead\" ) assert single == \"di no saur head\" multi = HyphenPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ] PhoneticPrep \u00b6 The ProneticPrep object prepares strings by encoding them phonetically. Parameters: Name Type Description Default kind type of encoding, either \"soundex\" , \" metaphone \" or \"nysiis\" required Usage: from tokenwiser.prep import PhoneticPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = PhoneticPrep ( kind = \"soundex\" ) . encode_single ( \"dinosaurus book\" ) example2 = PhoneticPrep ( kind = \"metaphone\" ) . encode_single ( \"dinosaurus book\" ) example3 = PhoneticPrep ( kind = \"nysiis\" ) . encode_single ( \"dinosaurus book\" ) assert example1 == 'D526 B200' assert example2 == 'TNSRS BK' assert example3 == 'DANASAR BAC' SpacyMorphPrep \u00b6 Adds morphologic information to tokens in text. Usage: import spacy from tokenwiser.prep import SpacyMorphPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyMorphPrep ( nlp ) . encode_single ( \"quick! duck!\" ) example2 = SpacyMorphPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"quick|Degree=Pos !|PunctType=Peri duck|Number=Sing !|PunctType=Peri\" assert example2 == \"hey| look|VerbForm=Inf a|Definite=Ind|PronType=Art duck|Number=Sing\" SpacyPosPrep \u00b6 Adds part of speech information per token using spaCy. Parameters: Name Type Description Default model the spaCy model to use required lemma also lemmatize the text required fine_grained use fine grained parts of speech required Usage: import spacy from tokenwiser.prep import SpacyPosPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyPosPrep ( nlp ) . encode_single ( \"we need to duck\" ) example2 = SpacyPosPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"we|PRON need|VERB to|PART duck|VERB\" assert example2 == \"hey|INTJ look|VERB a|DET duck|NOUN\" SpacyLemmaPrep \u00b6 Turns each token into a lemmatizer version using spaCy. Usage: import spacy from tokenwiser.prep import SpacyLemmaPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyLemmaPrep ( nlp ) . encode_single ( \"we are running\" ) example2 = SpacyLemmaPrep ( nlp ) . encode_single ( \"these are dogs\" ) assert example1 == 'we be run' assert example2 == 'these be dog' TextConcat \u00b6 A component like FeatureUnion but this also concatenates the text. Parameters: Name Type Description Default transformer_list list of (name, text-transformer)-tuples required Examples: from tokenwiser.prep import HyphenPrep , Cleaner , TextConcat tc = TextConcat ([( \"hyp\" , HyphenPrep ()), ( \"clean\" , Cleaner ())]) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected fit ( self , X , y = None ) \u00b6 Fits the components. Parameters: Name Type Description Default X list of text, to be transformer required y a label, will be handled by the Pipeline -API None Source code in tokenwiser/prep/_concat.py def fit ( self , X , y = None ): \"\"\" Fits the components. Arguments: X: list of text, to be transformer y: a label, will be handled by the `Pipeline`-API \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) return self fit_transform ( self , X , y = None ) \u00b6 Fits the components and transforms the text in one step. Parameters: Name Type Description Default X list of text, to be transformer required y a label, will be handled by the Pipeline -API None Source code in tokenwiser/prep/_concat.py def fit_transform ( self , X , y = None ): \"\"\" Fits the components and transforms the text in one step. Arguments: X: list of text, to be transformer y: a label, will be handled by the `Pipeline`-API \"\"\" return self . fit ( X , y ) . transform ( X , y ) transform ( self , X , y = None ) \u00b6 Transformers the text. Parameters: Name Type Description Default X list of text, to be transformer required y a label, will be handled by the Pipeline -API None Source code in tokenwiser/prep/_concat.py def transform ( self , X , y = None ): \"\"\" Transformers the text. Arguments: X: list of text, to be transformer y: a label, will be handled by the `Pipeline`-API \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) results = {} for name , tfm in self . transformer_list : results [ name ] = tfm . transform ( X ) return [ \" \" . join ([ results [ n ][ i ] for n in names ]) for i in range ( len ( X ))] make_concat ( * steps ) \u00b6 Utility function to generate a TextConcat Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.prep import HyphenPrep , Cleaner , make_concat tc = make_concat ( HyphenPrep (), Cleaner ()) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected Source code in tokenwiser/prep/_concat.py def make_concat ( * steps ): \"\"\" Utility function to generate a `TextConcat` Arguments: steps: a collection of text-transformers ```python from tokenwiser.prep import HyphenPrep, Cleaner, make_concat tc = make_concat(HyphenPrep(), Cleaner()) results = tc.fit_transform([\"dinosaurhead\", \"another$$ sentence$$\"]) expected = ['di no saur head dinosaurhead', 'an other $$ sen tence$$ another sentence'] assert results == expected ``` \"\"\" return TextConcat ( _name_estimators ( steps ))","title":"prep"},{"location":"prep.html#tokenisers","text":"Tokenisers are pipeline components that take a string and output a list of strings.","title":"tokenisers"},{"location":"prep.html#tokenwiser.prep._cleaner.Cleaner","text":"Applies a lowercase and removes non-alphanum. Usage: from tokenwiser.prep import Cleaner single = Cleaner () . encode_single ( \"$$$5 dollars\" ) assert single == \"5 dollars\" multi = Cleaner () . transform ([ \"$$$5 dollars\" , \"#hashtag!\" ]) assert multi == [ \"5 dollars\" , \"hashtag\" ]","title":"Cleaner"},{"location":"prep.html#tokenwiser.prep._hyphen.HyphenPrep","text":"Hyphenate the text going in. Usage: from tokenwiser.prep import HyphenPrep single = HyphenPrep () . encode_single ( \"dinosaurhead\" ) assert single == \"di no saur head\" multi = HyphenPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ]","title":"HyphenPrep"},{"location":"prep.html#tokenwiser.prep._phonetic.PhoneticPrep","text":"The ProneticPrep object prepares strings by encoding them phonetically. Parameters: Name Type Description Default kind type of encoding, either \"soundex\" , \" metaphone \" or \"nysiis\" required Usage: from tokenwiser.prep import PhoneticPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = PhoneticPrep ( kind = \"soundex\" ) . encode_single ( \"dinosaurus book\" ) example2 = PhoneticPrep ( kind = \"metaphone\" ) . encode_single ( \"dinosaurus book\" ) example3 = PhoneticPrep ( kind = \"nysiis\" ) . encode_single ( \"dinosaurus book\" ) assert example1 == 'D526 B200' assert example2 == 'TNSRS BK' assert example3 == 'DANASAR BAC'","title":"PhoneticPrep"},{"location":"prep.html#tokenwiser.prep._morph.SpacyMorphPrep","text":"Adds morphologic information to tokens in text. Usage: import spacy from tokenwiser.prep import SpacyMorphPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyMorphPrep ( nlp ) . encode_single ( \"quick! duck!\" ) example2 = SpacyMorphPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"quick|Degree=Pos !|PunctType=Peri duck|Number=Sing !|PunctType=Peri\" assert example2 == \"hey| look|VerbForm=Inf a|Definite=Ind|PronType=Art duck|Number=Sing\"","title":"SpacyMorphPrep"},{"location":"prep.html#tokenwiser.prep._morph.SpacyPosPrep","text":"Adds part of speech information per token using spaCy. Parameters: Name Type Description Default model the spaCy model to use required lemma also lemmatize the text required fine_grained use fine grained parts of speech required Usage: import spacy from tokenwiser.prep import SpacyPosPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyPosPrep ( nlp ) . encode_single ( \"we need to duck\" ) example2 = SpacyPosPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"we|PRON need|VERB to|PART duck|VERB\" assert example2 == \"hey|INTJ look|VERB a|DET duck|NOUN\"","title":"SpacyPosPrep"},{"location":"prep.html#tokenwiser.prep._morph.SpacyLemmaPrep","text":"Turns each token into a lemmatizer version using spaCy. Usage: import spacy from tokenwiser.prep import SpacyLemmaPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyLemmaPrep ( nlp ) . encode_single ( \"we are running\" ) example2 = SpacyLemmaPrep ( nlp ) . encode_single ( \"these are dogs\" ) assert example1 == 'we be run' assert example2 == 'these be dog'","title":"SpacyLemmaPrep"},{"location":"prep.html#tokenwiser.prep._concat.TextConcat","text":"A component like FeatureUnion but this also concatenates the text. Parameters: Name Type Description Default transformer_list list of (name, text-transformer)-tuples required Examples: from tokenwiser.prep import HyphenPrep , Cleaner , TextConcat tc = TextConcat ([( \"hyp\" , HyphenPrep ()), ( \"clean\" , Cleaner ())]) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected","title":"TextConcat"},{"location":"prep.html#tokenwiser.prep._concat.TextConcat.fit","text":"Fits the components. Parameters: Name Type Description Default X list of text, to be transformer required y a label, will be handled by the Pipeline -API None Source code in tokenwiser/prep/_concat.py def fit ( self , X , y = None ): \"\"\" Fits the components. Arguments: X: list of text, to be transformer y: a label, will be handled by the `Pipeline`-API \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) return self","title":"fit()"},{"location":"prep.html#tokenwiser.prep._concat.TextConcat.fit_transform","text":"Fits the components and transforms the text in one step. Parameters: Name Type Description Default X list of text, to be transformer required y a label, will be handled by the Pipeline -API None Source code in tokenwiser/prep/_concat.py def fit_transform ( self , X , y = None ): \"\"\" Fits the components and transforms the text in one step. Arguments: X: list of text, to be transformer y: a label, will be handled by the `Pipeline`-API \"\"\" return self . fit ( X , y ) . transform ( X , y )","title":"fit_transform()"},{"location":"prep.html#tokenwiser.prep._concat.TextConcat.transform","text":"Transformers the text. Parameters: Name Type Description Default X list of text, to be transformer required y a label, will be handled by the Pipeline -API None Source code in tokenwiser/prep/_concat.py def transform ( self , X , y = None ): \"\"\" Transformers the text. Arguments: X: list of text, to be transformer y: a label, will be handled by the `Pipeline`-API \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( f \"Make sure that the names of each step are unique.\" ) results = {} for name , tfm in self . transformer_list : results [ name ] = tfm . transform ( X ) return [ \" \" . join ([ results [ n ][ i ] for n in names ]) for i in range ( len ( X ))]","title":"transform()"},{"location":"prep.html#tokenwiser.prep._concat.make_concat","text":"Utility function to generate a TextConcat Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.prep import HyphenPrep , Cleaner , make_concat tc = make_concat ( HyphenPrep (), Cleaner ()) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected Source code in tokenwiser/prep/_concat.py def make_concat ( * steps ): \"\"\" Utility function to generate a `TextConcat` Arguments: steps: a collection of text-transformers ```python from tokenwiser.prep import HyphenPrep, Cleaner, make_concat tc = make_concat(HyphenPrep(), Cleaner()) results = tc.fit_transform([\"dinosaurhead\", \"another$$ sentence$$\"]) expected = ['di no saur head dinosaurhead', 'an other $$ sen tence$$ another sentence'] assert results == expected ``` \"\"\" return TextConcat ( _name_estimators ( steps ))","title":"make_concat()"},{"location":"snippets.html","text":"Here's a few useful snippets to keep around. Scikit-Learn in SpaCy \u00b6 You can add a custom component if you're really keen. import spacy import pandas as pd from spacy.language import Language from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline , make_union df = pd . read_json ( \"data/intents.jsonl\" , lines = True ) feat = make_union ( CountVectorizer ()) pipe = make_pipeline ( feat , LogisticRegression ()) pipe . fit ( df [ 'text' ], df [ 'label' ]) nlp = spacy . load ( \"en_core_web_md\" ) @Language . component ( \"sklearn-cat\" ) def my_component ( doc ): pred = pipe . predict ([ doc . text ])[ 0 ] proba = pipe . predict_proba ([ doc . text ]) . max () doc . cats [ pred ] = proba return doc nlp . add_pipe ( \"sklearn-cat\" ) nlp ( \"can you flip a coin?\" ) . cats # {'flip_coin': 0.9747356912446946}","title":"Snippets"},{"location":"snippets.html#scikit-learn-in-spacy","text":"You can add a custom component if you're really keen. import spacy import pandas as pd from spacy.language import Language from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline , make_union df = pd . read_json ( \"data/intents.jsonl\" , lines = True ) feat = make_union ( CountVectorizer ()) pipe = make_pipeline ( feat , LogisticRegression ()) pipe . fit ( df [ 'text' ], df [ 'label' ]) nlp = spacy . load ( \"en_core_web_md\" ) @Language . component ( \"sklearn-cat\" ) def my_component ( doc ): pred = pipe . predict ([ doc . text ])[ 0 ] proba = pipe . predict_proba ([ doc . text ]) . max () doc . cats [ pred ] = proba return doc nlp . add_pipe ( \"sklearn-cat\" ) nlp ( \"can you flip a coin?\" ) . cats # {'flip_coin': 0.9747356912446946}","title":"Scikit-Learn in SpaCy"}]}